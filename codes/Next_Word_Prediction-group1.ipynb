{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data/praire-clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened my eyes and saw a pea green world all around me  Then I heard the doctor say   Give  er another whiff or two   His voice sounded far away  as though he were speaking through the Simplon Tunnel  and not merely through his teeth  within twelve inches of my nose   I took my whiff or two  I gulped at that chloroform like a thirsty Bedouin at a wadi spring  I went down into the pea green emptiness again  and forgot about the Kelly pad and the recurring waves of pain that came bigger and bigg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened my eyes and saw a pea-green world all around me. Then heard the doctor say: \"Give \\'er another whiff or two.\" His voice sounded far-away, as though he were speaking through Simplon Tunnel, not merely his teeth, within twelve inches of nose. took two. gulped at that chloroform like thirsty Bedouin wadi-spring. went down into emptiness again, forgot about Kelly pad recurring waves pain came bigger tried to sweep racked old body breakers ribs stranded schooner. hateful metallic clink steel '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 2618, 403, 404, 1, 657, 24, 2619, 87, 88]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('../model/tokenizer_avi.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8595\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [  31 2618  403  404    1]\n",
      "The responses are:  [2618  403  404    1  657]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1, 10)             85950     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8595)              8603595   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,738,545\n",
      "Trainable params: 21,738,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"../model/nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),metrics=[perplexity,\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 04:34:19.017966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:34:19.565865: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:34:19.650948: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:34:20.100582: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:34:20.216352: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - ETA: 0s - loss: 9.0587 - perplexity: 533.2825 - accuracy: 0.0025\n",
      "Epoch 1: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 7s 245ms/step - loss: 9.0587 - perplexity: 533.2825 - accuracy: 0.0025 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 9.0218 - perplexity: 527.1422 - accuracy: 0.0035\n",
      "Epoch 2: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 9.0218 - perplexity: 527.1422 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.8889 - perplexity: 547.8386 - accuracy: 0.0035\n",
      "Epoch 3: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 8.8889 - perplexity: 547.8386 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.8306 - perplexity: 516.0404 - accuracy: 0.0035\n",
      "Epoch 4: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 8.8306 - perplexity: 516.0404 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.6909 - perplexity: 494.3646 - accuracy: 0.0035\n",
      "Epoch 5: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 8.6909 - perplexity: 494.3646 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.5544 - perplexity: 469.3459 - accuracy: 0.0036\n",
      "Epoch 6: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 195ms/step - loss: 8.5544 - perplexity: 469.3459 - accuracy: 0.0036 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.4420 - perplexity: 433.3560 - accuracy: 0.0031\n",
      "Epoch 7: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 195ms/step - loss: 8.4420 - perplexity: 433.3560 - accuracy: 0.0031 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.3283 - perplexity: 406.6493 - accuracy: 0.0031\n",
      "Epoch 8: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 195ms/step - loss: 8.3283 - perplexity: 406.6493 - accuracy: 0.0031 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.2596 - perplexity: 377.5417 - accuracy: 0.0037\n",
      "Epoch 9: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 192ms/step - loss: 8.2596 - perplexity: 377.5417 - accuracy: 0.0037 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.1836 - perplexity: 353.7816 - accuracy: 0.0030\n",
      "Epoch 10: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 8.1836 - perplexity: 353.7816 - accuracy: 0.0030 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.1307 - perplexity: 339.8178 - accuracy: 0.0028\n",
      "Epoch 11: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 240ms/step - loss: 8.1307 - perplexity: 339.8178 - accuracy: 0.0028 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0936 - perplexity: 330.0387 - accuracy: 0.0028\n",
      "Epoch 12: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 294ms/step - loss: 8.0936 - perplexity: 330.0387 - accuracy: 0.0028 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0596 - perplexity: 320.8770 - accuracy: 0.0030\n",
      "Epoch 13: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 307ms/step - loss: 8.0596 - perplexity: 320.8770 - accuracy: 0.0030 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0253 - perplexity: 312.8816 - accuracy: 0.0038\n",
      "Epoch 14: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 7s 435ms/step - loss: 8.0253 - perplexity: 312.8816 - accuracy: 0.0038 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.9845 - perplexity: 304.4140 - accuracy: 0.0039\n",
      "Epoch 15: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 227ms/step - loss: 7.9845 - perplexity: 304.4140 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.9271 - perplexity: 292.6863 - accuracy: 0.0037\n",
      "Epoch 16: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 7.9271 - perplexity: 292.6863 - accuracy: 0.0037 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.8599 - perplexity: 281.5545 - accuracy: 0.0039\n",
      "Epoch 17: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 219ms/step - loss: 7.8599 - perplexity: 281.5545 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.7766 - perplexity: 266.2371 - accuracy: 0.0044\n",
      "Epoch 18: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 7.7766 - perplexity: 266.2371 - accuracy: 0.0044 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.6737 - perplexity: 248.1058 - accuracy: 0.0054\n",
      "Epoch 19: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 216ms/step - loss: 7.6737 - perplexity: 248.1058 - accuracy: 0.0054 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.5870 - perplexity: 233.7144 - accuracy: 0.0037\n",
      "Epoch 20: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 7.5870 - perplexity: 233.7144 - accuracy: 0.0037 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.4986 - perplexity: 219.9167 - accuracy: 0.0054\n",
      "Epoch 21: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 7.4986 - perplexity: 219.9167 - accuracy: 0.0054 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.4303 - perplexity: 209.8718 - accuracy: 0.0058\n",
      "Epoch 22: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 7.4303 - perplexity: 209.8718 - accuracy: 0.0058 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.3722 - perplexity: 201.3366 - accuracy: 0.0066\n",
      "Epoch 23: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 7.3722 - perplexity: 201.3366 - accuracy: 0.0066 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.3148 - perplexity: 193.4631 - accuracy: 0.0062\n",
      "Epoch 24: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 178ms/step - loss: 7.3148 - perplexity: 193.4631 - accuracy: 0.0062 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.2626 - perplexity: 187.5853 - accuracy: 0.0061\n",
      "Epoch 25: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 176ms/step - loss: 7.2626 - perplexity: 187.5853 - accuracy: 0.0061 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.2172 - perplexity: 181.7368 - accuracy: 0.0063\n",
      "Epoch 26: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 179ms/step - loss: 7.2172 - perplexity: 181.7368 - accuracy: 0.0063 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1667 - perplexity: 176.2819 - accuracy: 0.0064\n",
      "Epoch 27: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 175ms/step - loss: 7.1667 - perplexity: 176.2819 - accuracy: 0.0064 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1348 - perplexity: 172.3197 - accuracy: 0.0072\n",
      "Epoch 28: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 174ms/step - loss: 7.1348 - perplexity: 172.3197 - accuracy: 0.0072 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1086 - perplexity: 170.2815 - accuracy: 0.0067\n",
      "Epoch 29: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 177ms/step - loss: 7.1086 - perplexity: 170.2815 - accuracy: 0.0067 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0829 - perplexity: 165.9184 - accuracy: 0.0081\n",
      "Epoch 30: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 175ms/step - loss: 7.0829 - perplexity: 165.9184 - accuracy: 0.0081 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0408 - perplexity: 160.9140 - accuracy: 0.0077\n",
      "Epoch 31: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 176ms/step - loss: 7.0408 - perplexity: 160.9140 - accuracy: 0.0077 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9986 - perplexity: 156.5685 - accuracy: 0.0081\n",
      "Epoch 32: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 175ms/step - loss: 6.9986 - perplexity: 156.5685 - accuracy: 0.0081 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9775 - perplexity: 155.3149 - accuracy: 0.0066\n",
      "Epoch 33: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 196ms/step - loss: 6.9775 - perplexity: 155.3149 - accuracy: 0.0066 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9567 - perplexity: 153.0643 - accuracy: 0.0079\n",
      "Epoch 34: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 6.9567 - perplexity: 153.0643 - accuracy: 0.0079 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9214 - perplexity: 149.5088 - accuracy: 0.0075\n",
      "Epoch 35: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 6s 407ms/step - loss: 6.9214 - perplexity: 149.5088 - accuracy: 0.0075 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8928 - perplexity: 146.9108 - accuracy: 0.0080\n",
      "Epoch 36: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 227ms/step - loss: 6.8928 - perplexity: 146.9108 - accuracy: 0.0080 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8721 - perplexity: 144.2597 - accuracy: 0.0087\n",
      "Epoch 37: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 281ms/step - loss: 6.8721 - perplexity: 144.2597 - accuracy: 0.0087 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8506 - perplexity: 143.3372 - accuracy: 0.0081\n",
      "Epoch 38: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 218ms/step - loss: 6.8506 - perplexity: 143.3372 - accuracy: 0.0081 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8275 - perplexity: 139.7628 - accuracy: 0.0096\n",
      "Epoch 39: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 6.8275 - perplexity: 139.7628 - accuracy: 0.0096 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8058 - perplexity: 138.4875 - accuracy: 0.0094\n",
      "Epoch 40: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 259ms/step - loss: 6.8058 - perplexity: 138.4875 - accuracy: 0.0094 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7864 - perplexity: 137.6360 - accuracy: 0.0091\n",
      "Epoch 41: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 6.7864 - perplexity: 137.6360 - accuracy: 0.0091 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7699 - perplexity: 135.5545 - accuracy: 0.0087\n",
      "Epoch 42: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 215ms/step - loss: 6.7699 - perplexity: 135.5545 - accuracy: 0.0087 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7504 - perplexity: 140.9735 - accuracy: 0.0095\n",
      "Epoch 43: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 6.7504 - perplexity: 140.9735 - accuracy: 0.0095 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7376 - perplexity: 131.3037 - accuracy: 0.0094\n",
      "Epoch 44: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 251ms/step - loss: 6.7376 - perplexity: 131.3037 - accuracy: 0.0094 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7309 - perplexity: 131.0283 - accuracy: 0.0103\n",
      "Epoch 45: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 6.7309 - perplexity: 131.0283 - accuracy: 0.0103 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7160 - perplexity: 132.2079 - accuracy: 0.0087\n",
      "Epoch 46: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 301ms/step - loss: 6.7160 - perplexity: 132.2079 - accuracy: 0.0087 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7060 - perplexity: 130.9553 - accuracy: 0.0085\n",
      "Epoch 47: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 6.7060 - perplexity: 130.9553 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6912 - perplexity: 129.8156 - accuracy: 0.0090\n",
      "Epoch 48: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 264ms/step - loss: 6.6912 - perplexity: 129.8156 - accuracy: 0.0090 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6692 - perplexity: 127.3227 - accuracy: 0.0084\n",
      "Epoch 49: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 263ms/step - loss: 6.6692 - perplexity: 127.3227 - accuracy: 0.0084 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6481 - perplexity: 127.8183 - accuracy: 0.0092\n",
      "Epoch 50: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 218ms/step - loss: 6.6481 - perplexity: 127.8183 - accuracy: 0.0092 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6212 - perplexity: 122.1225 - accuracy: 0.0096\n",
      "Epoch 51: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 216ms/step - loss: 6.6212 - perplexity: 122.1225 - accuracy: 0.0096 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6039 - perplexity: 120.8848 - accuracy: 0.0094\n",
      "Epoch 52: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 176ms/step - loss: 6.6039 - perplexity: 120.8848 - accuracy: 0.0094 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5982 - perplexity: 125.2564 - accuracy: 0.0107\n",
      "Epoch 53: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 221ms/step - loss: 6.5982 - perplexity: 125.2564 - accuracy: 0.0107 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5935 - perplexity: 123.3295 - accuracy: 0.0100\n",
      "Epoch 54: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 222ms/step - loss: 6.5935 - perplexity: 123.3295 - accuracy: 0.0100 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5740 - perplexity: 119.0006 - accuracy: 0.0085\n",
      "Epoch 55: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 333ms/step - loss: 6.5740 - perplexity: 119.0006 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5563 - perplexity: 118.9703 - accuracy: 0.0104\n",
      "Epoch 56: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 301ms/step - loss: 6.5563 - perplexity: 118.9703 - accuracy: 0.0104 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5451 - perplexity: 117.9609 - accuracy: 0.0103\n",
      "Epoch 57: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 6.5451 - perplexity: 117.9609 - accuracy: 0.0103 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5541 - perplexity: 130.7764 - accuracy: 0.0104\n",
      "Epoch 58: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 229ms/step - loss: 6.5541 - perplexity: 130.7764 - accuracy: 0.0104 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5411 - perplexity: 119.4908 - accuracy: 0.0112\n",
      "Epoch 59: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 222ms/step - loss: 6.5411 - perplexity: 119.4908 - accuracy: 0.0112 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5110 - perplexity: 115.2439 - accuracy: 0.0112\n",
      "Epoch 60: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 6.5110 - perplexity: 115.2439 - accuracy: 0.0112 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5021 - perplexity: 122.4786 - accuracy: 0.0111\n",
      "Epoch 61: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 225ms/step - loss: 6.5021 - perplexity: 122.4786 - accuracy: 0.0111 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4894 - perplexity: 129.3945 - accuracy: 0.0104\n",
      "Epoch 62: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 6.4894 - perplexity: 129.3945 - accuracy: 0.0104 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4784 - perplexity: 117.5660 - accuracy: 0.0107\n",
      "Epoch 63: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 217ms/step - loss: 6.4784 - perplexity: 117.5660 - accuracy: 0.0107 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4706 - perplexity: 113.2768 - accuracy: 0.0113\n",
      "Epoch 64: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 6.4706 - perplexity: 113.2768 - accuracy: 0.0113 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4731 - perplexity: 116.3096 - accuracy: 0.0113\n",
      "Epoch 65: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 219ms/step - loss: 6.4731 - perplexity: 116.3096 - accuracy: 0.0113 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4617 - perplexity: 123.0897 - accuracy: 0.0102\n",
      "Epoch 66: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 219ms/step - loss: 6.4617 - perplexity: 123.0897 - accuracy: 0.0102 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4479 - perplexity: 122.6073 - accuracy: 0.0115\n",
      "Epoch 67: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 236ms/step - loss: 6.4479 - perplexity: 122.6073 - accuracy: 0.0115 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4412 - perplexity: 129.7668 - accuracy: 0.0105\n",
      "Epoch 68: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 222ms/step - loss: 6.4412 - perplexity: 129.7668 - accuracy: 0.0105 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4141 - perplexity: 114.0964 - accuracy: 0.0128\n",
      "Epoch 69: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 216ms/step - loss: 6.4141 - perplexity: 114.0964 - accuracy: 0.0128 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4073 - perplexity: 113.8813 - accuracy: 0.0120\n",
      "Epoch 70: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 206ms/step - loss: 6.4073 - perplexity: 113.8813 - accuracy: 0.0120 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4023 - perplexity: 109.5098 - accuracy: 0.0115\n",
      "Epoch 71: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 6.4023 - perplexity: 109.5098 - accuracy: 0.0115 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3968 - perplexity: 108.5055 - accuracy: 0.0127\n",
      "Epoch 72: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 212ms/step - loss: 6.3968 - perplexity: 108.5055 - accuracy: 0.0127 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3901 - perplexity: 112.5543 - accuracy: 0.0117\n",
      "Epoch 73: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 196ms/step - loss: 6.3901 - perplexity: 112.5543 - accuracy: 0.0117 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3800 - perplexity: 128.3108 - accuracy: 0.0123\n",
      "Epoch 74: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 6.3800 - perplexity: 128.3108 - accuracy: 0.0123 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3713 - perplexity: 141.6914 - accuracy: 0.0122\n",
      "Epoch 75: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 219ms/step - loss: 6.3713 - perplexity: 141.6914 - accuracy: 0.0122 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3560 - perplexity: 107.5706 - accuracy: 0.0128\n",
      "Epoch 76: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 243ms/step - loss: 6.3560 - perplexity: 107.5706 - accuracy: 0.0128 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3494 - perplexity: 110.5228 - accuracy: 0.0130\n",
      "Epoch 77: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 6s 424ms/step - loss: 6.3494 - perplexity: 110.5228 - accuracy: 0.0130 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3389 - perplexity: 117.3529 - accuracy: 0.0135\n",
      "Epoch 78: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 11s 563ms/step - loss: 6.3389 - perplexity: 117.3529 - accuracy: 0.0135 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3264 - perplexity: 107.2861 - accuracy: 0.0115\n",
      "Epoch 79: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 6s 387ms/step - loss: 6.3264 - perplexity: 107.2861 - accuracy: 0.0115 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3302 - perplexity: 112.9148 - accuracy: 0.0131\n",
      "Epoch 80: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 339ms/step - loss: 6.3302 - perplexity: 112.9148 - accuracy: 0.0131 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3113 - perplexity: 105.1442 - accuracy: 0.0128\n",
      "Epoch 81: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 230ms/step - loss: 6.3113 - perplexity: 105.1442 - accuracy: 0.0128 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2975 - perplexity: 133.7389 - accuracy: 0.0138\n",
      "Epoch 82: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 232ms/step - loss: 6.2975 - perplexity: 133.7389 - accuracy: 0.0138 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2925 - perplexity: 103.2462 - accuracy: 0.0136\n",
      "Epoch 83: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 273ms/step - loss: 6.2925 - perplexity: 103.2462 - accuracy: 0.0136 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2811 - perplexity: 196.6664 - accuracy: 0.0140\n",
      "Epoch 84: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 207ms/step - loss: 6.2811 - perplexity: 196.6664 - accuracy: 0.0140 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2768 - perplexity: 131.2092 - accuracy: 0.0127\n",
      "Epoch 85: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 224ms/step - loss: 6.2768 - perplexity: 131.2092 - accuracy: 0.0127 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2780 - perplexity: 158.7638 - accuracy: 0.0125\n",
      "Epoch 86: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 6.2780 - perplexity: 158.7638 - accuracy: 0.0125 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2748 - perplexity: 149.0539 - accuracy: 0.0136\n",
      "Epoch 87: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.2748 - perplexity: 149.0539 - accuracy: 0.0136 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2692 - perplexity: 170.4215 - accuracy: 0.0128\n",
      "Epoch 88: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 212ms/step - loss: 6.2692 - perplexity: 170.4215 - accuracy: 0.0128 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2495 - perplexity: 151.6017 - accuracy: 0.0138\n",
      "Epoch 89: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 216ms/step - loss: 6.2495 - perplexity: 151.6017 - accuracy: 0.0138 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2483 - perplexity: 108.1412 - accuracy: 0.0147\n",
      "Epoch 90: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 247ms/step - loss: 6.2483 - perplexity: 108.1412 - accuracy: 0.0147 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2363 - perplexity: 108.6651 - accuracy: 0.0140\n",
      "Epoch 91: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 252ms/step - loss: 6.2363 - perplexity: 108.6651 - accuracy: 0.0140 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2307 - perplexity: 154.9782 - accuracy: 0.0136\n",
      "Epoch 92: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 224ms/step - loss: 6.2307 - perplexity: 154.9782 - accuracy: 0.0136 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2272 - perplexity: 227.2757 - accuracy: 0.0152\n",
      "Epoch 93: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 218ms/step - loss: 6.2272 - perplexity: 227.2757 - accuracy: 0.0152 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2088 - perplexity: 139.9584 - accuracy: 0.0167\n",
      "Epoch 94: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 6.2088 - perplexity: 139.9584 - accuracy: 0.0167 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2153 - perplexity: 130.6752 - accuracy: 0.0151\n",
      "Epoch 95: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 230ms/step - loss: 6.2153 - perplexity: 130.6752 - accuracy: 0.0151 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1989 - perplexity: 131.6925 - accuracy: 0.0153\n",
      "Epoch 96: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 215ms/step - loss: 6.1989 - perplexity: 131.6925 - accuracy: 0.0153 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1738 - perplexity: 100.2457 - accuracy: 0.0147\n",
      "Epoch 97: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 227ms/step - loss: 6.1738 - perplexity: 100.2457 - accuracy: 0.0147 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1666 - perplexity: 233.1180 - accuracy: 0.0158\n",
      "Epoch 98: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 219ms/step - loss: 6.1666 - perplexity: 233.1180 - accuracy: 0.0158 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1681 - perplexity: 100.8626 - accuracy: 0.0164\n",
      "Epoch 99: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 231ms/step - loss: 6.1681 - perplexity: 100.8626 - accuracy: 0.0164 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1643 - perplexity: 103.2352 - accuracy: 0.0159\n",
      "Epoch 100: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 6.1643 - perplexity: 103.2352 - accuracy: 0.0159 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1584 - perplexity: 120.8477 - accuracy: 0.0159\n",
      "Epoch 101: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 226ms/step - loss: 6.1584 - perplexity: 120.8477 - accuracy: 0.0159 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1404 - perplexity: 103.1415 - accuracy: 0.0145\n",
      "Epoch 102: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 222ms/step - loss: 6.1404 - perplexity: 103.1415 - accuracy: 0.0145 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1316 - perplexity: 97.7690 - accuracy: 0.0156\n",
      "Epoch 103: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 260ms/step - loss: 6.1316 - perplexity: 97.7690 - accuracy: 0.0156 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1288 - perplexity: 129.8986 - accuracy: 0.0164\n",
      "Epoch 104: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 291ms/step - loss: 6.1288 - perplexity: 129.8986 - accuracy: 0.0164 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1181 - perplexity: 115.9966 - accuracy: 0.0180\n",
      "Epoch 105: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 211ms/step - loss: 6.1181 - perplexity: 115.9966 - accuracy: 0.0180 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0932 - perplexity: 109.4772 - accuracy: 0.0171\n",
      "Epoch 106: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 196ms/step - loss: 6.0932 - perplexity: 109.4772 - accuracy: 0.0171 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0767 - perplexity: 103.6390 - accuracy: 0.0176\n",
      "Epoch 107: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 6.0767 - perplexity: 103.6390 - accuracy: 0.0176 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0781 - perplexity: 108.1648 - accuracy: 0.0164\n",
      "Epoch 108: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 269ms/step - loss: 6.0781 - perplexity: 108.1648 - accuracy: 0.0164 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0587 - perplexity: 94.0882 - accuracy: 0.0172\n",
      "Epoch 109: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 215ms/step - loss: 6.0587 - perplexity: 94.0882 - accuracy: 0.0172 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0529 - perplexity: 132.3882 - accuracy: 0.0178\n",
      "Epoch 110: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 212ms/step - loss: 6.0529 - perplexity: 132.3882 - accuracy: 0.0178 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0368 - perplexity: 96.5608 - accuracy: 0.0172\n",
      "Epoch 111: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 206ms/step - loss: 6.0368 - perplexity: 96.5608 - accuracy: 0.0172 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0043 - perplexity: 197.6343 - accuracy: 0.0179\n",
      "Epoch 112: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 212ms/step - loss: 6.0043 - perplexity: 197.6343 - accuracy: 0.0179 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9854 - perplexity: 222.3919 - accuracy: 0.0187\n",
      "Epoch 113: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 3s 203ms/step - loss: 5.9854 - perplexity: 222.3919 - accuracy: 0.0187 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9733 - perplexity: 184.9764 - accuracy: 0.0197\n",
      "Epoch 114: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 5.9733 - perplexity: 184.9764 - accuracy: 0.0197 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9355 - perplexity: 138.6070 - accuracy: 0.0190\n",
      "Epoch 115: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 21s 1s/step - loss: 5.9355 - perplexity: 138.6070 - accuracy: 0.0190 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9121 - perplexity: 119.5246 - accuracy: 0.0214\n",
      "Epoch 116: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 23s 2s/step - loss: 5.9121 - perplexity: 119.5246 - accuracy: 0.0214 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9038 - perplexity: 307.2256 - accuracy: 0.0199\n",
      "Epoch 117: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 22s 2s/step - loss: 5.9038 - perplexity: 307.2256 - accuracy: 0.0199 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.8670 - perplexity: 256.6792 - accuracy: 0.0218\n",
      "Epoch 118: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 20s 1s/step - loss: 5.8670 - perplexity: 256.6792 - accuracy: 0.0218 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.8364 - perplexity: 1758.3142 - accuracy: 0.0222\n",
      "Epoch 119: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 16s 1s/step - loss: 5.8364 - perplexity: 1758.3142 - accuracy: 0.0222 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.8031 - perplexity: 101.7565 - accuracy: 0.0227\n",
      "Epoch 120: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 10s 673ms/step - loss: 5.8031 - perplexity: 101.7565 - accuracy: 0.0227 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.7806 - perplexity: 362.7772 - accuracy: 0.0216\n",
      "Epoch 121: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 260ms/step - loss: 5.7806 - perplexity: 362.7772 - accuracy: 0.0216 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.7322 - perplexity: 105.0039 - accuracy: 0.0223\n",
      "Epoch 122: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 284ms/step - loss: 5.7322 - perplexity: 105.0039 - accuracy: 0.0223 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.6893 - perplexity: 300.9062 - accuracy: 0.0240\n",
      "Epoch 123: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 342ms/step - loss: 5.6893 - perplexity: 300.9062 - accuracy: 0.0240 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.6449 - perplexity: 168.3771 - accuracy: 0.0280\n",
      "Epoch 124: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 361ms/step - loss: 5.6449 - perplexity: 168.3771 - accuracy: 0.0280 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.6096 - perplexity: 112.5888 - accuracy: 0.0267\n",
      "Epoch 125: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 251ms/step - loss: 5.6096 - perplexity: 112.5888 - accuracy: 0.0267 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.5778 - perplexity: 9815.7197 - accuracy: 0.0276 \n",
      "Epoch 126: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 239ms/step - loss: 5.5778 - perplexity: 9815.7197 - accuracy: 0.0276 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.5222 - perplexity: 184.9908 - accuracy: 0.0292\n",
      "Epoch 127: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 5s 338ms/step - loss: 5.5222 - perplexity: 184.9908 - accuracy: 0.0292 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.4631 - perplexity: 93.8236 - accuracy: 0.0329\n",
      "Epoch 128: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 262ms/step - loss: 5.4631 - perplexity: 93.8236 - accuracy: 0.0329 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.4068 - perplexity: 116.4243 - accuracy: 0.0315\n",
      "Epoch 129: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 234ms/step - loss: 5.4068 - perplexity: 116.4243 - accuracy: 0.0315 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.3666 - perplexity: 125.1527 - accuracy: 0.0330\n",
      "Epoch 130: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 9s 632ms/step - loss: 5.3666 - perplexity: 125.1527 - accuracy: 0.0330 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.3193 - perplexity: 200.2825 - accuracy: 0.0311\n",
      "Epoch 131: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 6s 354ms/step - loss: 5.3193 - perplexity: 200.2825 - accuracy: 0.0311 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.2944 - perplexity: 110.0998 - accuracy: 0.0377\n",
      "Epoch 132: loss did not improve from 5.26307\n",
      "15/15 [==============================] - 4s 254ms/step - loss: 5.2944 - perplexity: 110.0998 - accuracy: 0.0377 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.2528 - perplexity: 397.7610 - accuracy: 0.0381\n",
      "Epoch 133: loss improved from 5.26307 to 5.25285, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 5s 336ms/step - loss: 5.2528 - perplexity: 397.7610 - accuracy: 0.0381 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.2114 - perplexity: 6211.8950 - accuracy: 0.0394\n",
      "Epoch 134: loss improved from 5.25285 to 5.21145, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 6s 405ms/step - loss: 5.2114 - perplexity: 6211.8950 - accuracy: 0.0394 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.1518 - perplexity: 200.6548 - accuracy: 0.0465\n",
      "Epoch 135: loss improved from 5.21145 to 5.15181, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 9s 574ms/step - loss: 5.1518 - perplexity: 200.6548 - accuracy: 0.0465 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0844 - perplexity: 881.9680 - accuracy: 0.0445\n",
      "Epoch 136: loss improved from 5.15181 to 5.08442, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 7s 460ms/step - loss: 5.0844 - perplexity: 881.9680 - accuracy: 0.0445 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0579 - perplexity: 164.5067 - accuracy: 0.0453\n",
      "Epoch 137: loss improved from 5.08442 to 5.05788, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 6s 363ms/step - loss: 5.0579 - perplexity: 164.5067 - accuracy: 0.0453 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0286 - perplexity: 100.0271 - accuracy: 0.0463\n",
      "Epoch 138: loss improved from 5.05788 to 5.02864, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 6s 421ms/step - loss: 5.0286 - perplexity: 100.0271 - accuracy: 0.0463 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9841 - perplexity: 188.5636 - accuracy: 0.0493\n",
      "Epoch 139: loss improved from 5.02864 to 4.98411, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 6s 393ms/step - loss: 4.9841 - perplexity: 188.5636 - accuracy: 0.0493 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9418 - perplexity: 56.7605 - accuracy: 0.0518\n",
      "Epoch 140: loss improved from 4.98411 to 4.94175, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 5s 250ms/step - loss: 4.9418 - perplexity: 56.7605 - accuracy: 0.0518 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9255 - perplexity: 6319456.0000 - accuracy: 0.0521\n",
      "Epoch 141: loss improved from 4.94175 to 4.92552, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 4.9255 - perplexity: 6319456.0000 - accuracy: 0.0521 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8945 - perplexity: 530708.0625 - accuracy: 0.0546\n",
      "Epoch 142: loss improved from 4.92552 to 4.89448, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 4.8945 - perplexity: 530708.0625 - accuracy: 0.0546 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8534 - perplexity: 514.2214 - accuracy: 0.0550\n",
      "Epoch 143: loss improved from 4.89448 to 4.85335, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 214ms/step - loss: 4.8534 - perplexity: 514.2214 - accuracy: 0.0550 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8155 - perplexity: 339.5167 - accuracy: 0.0602\n",
      "Epoch 144: loss improved from 4.85335 to 4.81550, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 4.8155 - perplexity: 339.5167 - accuracy: 0.0602 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7917 - perplexity: 7391.3384 - accuracy: 0.0596\n",
      "Epoch 145: loss improved from 4.81550 to 4.79168, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 209ms/step - loss: 4.7917 - perplexity: 7391.3384 - accuracy: 0.0596 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7568 - perplexity: 488.7219 - accuracy: 0.0607\n",
      "Epoch 146: loss improved from 4.79168 to 4.75683, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 4s 236ms/step - loss: 4.7568 - perplexity: 488.7219 - accuracy: 0.0607 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7185 - perplexity: 2194.2942 - accuracy: 0.0614\n",
      "Epoch 147: loss improved from 4.75683 to 4.71847, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 4s 253ms/step - loss: 4.7185 - perplexity: 2194.2942 - accuracy: 0.0614 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6698 - perplexity: 54.8509 - accuracy: 0.0681\n",
      "Epoch 148: loss improved from 4.71847 to 4.66978, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 6s 390ms/step - loss: 4.6698 - perplexity: 54.8509 - accuracy: 0.0681 - lr: 0.0010\n",
      "Epoch 149/200\n",
      " 4/15 [=======>......................] - ETA: 2s - loss: 4.2975 - perplexity: 31.5774 - accuracy: 0.1028"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X, y, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[checkpoint, reduce, tensorboard_Visualization])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/anly-580/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=150, batch_size=1000, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelCheckpoint' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# list all data in history\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(checkpoint\u001b[39m.\u001b[39;49mhistory\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelCheckpoint' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(checkpoint.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"../model/nextword.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../model/nextword.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE REFERENCE: \n",
    "https://www.kaggle.com/code/ysthehurricane/next-word-prediction-bi-lstm-tutorial-easy-way <br>\n",
    "https://www.analyticsvidhya.com/blog/2021/08/predict-the-next-word-of-your-text-using-long-short-term-memory-lstm/ <br>\n",
    "https://github.com/Bharath-K3/Next-Word-Prediction-with-NLP-and-Deep-Learning <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 02:55:11.370057: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = pickle.load(open('../model/tokenizer1.pkl', 'rb'))\n",
    "# load json and create model\n",
    "json_file = open('../model/nextword.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../model/nextword.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('anly-580')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5abdb303892873824d5077e7ac4d83088b903b32bdc9644b35b87d07f0c1fc70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
