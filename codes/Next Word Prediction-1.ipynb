{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  I opened my eyes and saw a pea-green world all around me. Then I heard\n",
      "\n",
      "The Last Line:  just what had kept him away so long!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
    "    Remove all the unnecessary data and label it as Metamorphosis-clean.\n",
    "    The starting and ending lines should be as follows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file = open(\"../data/praire-clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines=lines[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened my eyes and saw a pea-green world all around me. Then I heard\\n the doctor say: \"Give \\'er another whiff or two.\" His voice sounded\\n far-away, as though he were speaking through the Simplon Tunnel, and\\n not merely through his teeth, within twelve inches of my nose.\\n \\n I took my whiff or two. I gulped at that chloroform like a thirsty\\n Bedouin at a wad'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "#data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened my eyes and saw a pea green world all around me  Then I heard\\n the doctor say   Give  er another whiff or two   His voice sounded\\n far away  as though he were speaking through the Simplon Tunnel  and\\n not merely through his teeth  within twelve inches of my nose \\n \\n I took my whiff or two  I gulped at that chloroform like a thirsty\\n Bedouin at a wadi spring  I went down into the pea green emptiness\\n again  and forgot about the Kelly pad and the recurring waves of pain\\n that came bigger '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened my eyes and saw a pea-green world all around me. Then heard the doctor say: \"Give \\'er another whiff or two.\" His voice sounded far-away, as though he were speaking through Simplon Tunnel, not merely his teeth, within twelve inches of nose. took two. gulped at that chloroform like thirsty Bedouin wadi-spring. went down into emptiness again, forgot about Kelly pad recurring waves pain came bigger tried to sweep racked old body breakers ribs stranded schooner. hateful metallic clink steel '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 2618, 403, 404, 1, 657, 24, 2619, 87, 88]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8595\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  14737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  31, 2618],\n",
       "       [2618,  403],\n",
       "       [ 403,  404],\n",
       "       [ 404,    1],\n",
       "       [   1,  657],\n",
       "       [ 657,   24],\n",
       "       [  24, 2619],\n",
       "       [2619,   87],\n",
       "       [  87,   88],\n",
       "       [  88,  146]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [  31 2618  403  404    1]\n",
      "The responses are:  [2618  403  404    1  657]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 14:05:47.579601: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             85950     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8595)              8603595   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,738,545\n",
      "Trainable params: 21,738,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 9.0589 - accuracy: 0.0016\n",
      "Epoch 1: loss improved from inf to 9.05887, saving model to nextword1.h5\n",
      "15/15 [==============================] - 16s 779ms/step - loss: 9.0589 - accuracy: 0.0016 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 9.0266 - accuracy: 0.0035\n",
      "Epoch 2: loss improved from 9.05887 to 9.02657, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 795ms/step - loss: 9.0266 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.8989 - accuracy: 0.0033\n",
      "Epoch 3: loss improved from 9.02657 to 8.89885, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 731ms/step - loss: 8.8989 - accuracy: 0.0033 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.8429 - accuracy: 0.0035\n",
      "Epoch 4: loss improved from 8.89885 to 8.84294, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 731ms/step - loss: 8.8429 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.7270 - accuracy: 0.0035\n",
      "Epoch 5: loss improved from 8.84294 to 8.72699, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 852ms/step - loss: 8.7270 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.5857 - accuracy: 0.0035\n",
      "Epoch 6: loss improved from 8.72699 to 8.58571, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 780ms/step - loss: 8.5857 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.4697 - accuracy: 0.0033\n",
      "Epoch 7: loss improved from 8.58571 to 8.46972, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 801ms/step - loss: 8.4697 - accuracy: 0.0033 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.3480 - accuracy: 0.0026\n",
      "Epoch 8: loss improved from 8.46972 to 8.34805, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 807ms/step - loss: 8.3480 - accuracy: 0.0026 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.2578 - accuracy: 0.0027\n",
      "Epoch 9: loss improved from 8.34805 to 8.25778, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 774ms/step - loss: 8.2578 - accuracy: 0.0027 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.1900 - accuracy: 0.0030\n",
      "Epoch 10: loss improved from 8.25778 to 8.19002, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 777ms/step - loss: 8.1900 - accuracy: 0.0030 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.1324 - accuracy: 0.0032\n",
      "Epoch 11: loss improved from 8.19002 to 8.13241, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 750ms/step - loss: 8.1324 - accuracy: 0.0032 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0949 - accuracy: 0.0030\n",
      "Epoch 12: loss improved from 8.13241 to 8.09494, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 751ms/step - loss: 8.0949 - accuracy: 0.0030 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0590 - accuracy: 0.0031\n",
      "Epoch 13: loss improved from 8.09494 to 8.05898, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 763ms/step - loss: 8.0590 - accuracy: 0.0031 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0280 - accuracy: 0.0031\n",
      "Epoch 14: loss improved from 8.05898 to 8.02804, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 751ms/step - loss: 8.0280 - accuracy: 0.0031 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.9885 - accuracy: 0.0030\n",
      "Epoch 15: loss improved from 8.02804 to 7.98847, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 788ms/step - loss: 7.9885 - accuracy: 0.0030 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.9438 - accuracy: 0.0039\n",
      "Epoch 16: loss improved from 7.98847 to 7.94378, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 870ms/step - loss: 7.9438 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.8803 - accuracy: 0.0048\n",
      "Epoch 17: loss improved from 7.94378 to 7.88029, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 851ms/step - loss: 7.8803 - accuracy: 0.0048 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.8004 - accuracy: 0.0044\n",
      "Epoch 18: loss improved from 7.88029 to 7.80036, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 811ms/step - loss: 7.8004 - accuracy: 0.0044 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.7097 - accuracy: 0.0052\n",
      "Epoch 19: loss improved from 7.80036 to 7.70970, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 794ms/step - loss: 7.7097 - accuracy: 0.0052 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.6177 - accuracy: 0.0050\n",
      "Epoch 20: loss improved from 7.70970 to 7.61771, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 780ms/step - loss: 7.6177 - accuracy: 0.0050 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.5309 - accuracy: 0.0054\n",
      "Epoch 21: loss improved from 7.61771 to 7.53088, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 766ms/step - loss: 7.5309 - accuracy: 0.0054 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.4558 - accuracy: 0.0051\n",
      "Epoch 22: loss improved from 7.53088 to 7.45580, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 763ms/step - loss: 7.4558 - accuracy: 0.0051 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.3872 - accuracy: 0.0063\n",
      "Epoch 23: loss improved from 7.45580 to 7.38719, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 828ms/step - loss: 7.3872 - accuracy: 0.0063 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.3290 - accuracy: 0.0058\n",
      "Epoch 24: loss improved from 7.38719 to 7.32895, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 778ms/step - loss: 7.3290 - accuracy: 0.0058 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.2808 - accuracy: 0.0051\n",
      "Epoch 25: loss improved from 7.32895 to 7.28084, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 825ms/step - loss: 7.2808 - accuracy: 0.0051 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.2307 - accuracy: 0.0054\n",
      "Epoch 26: loss improved from 7.28084 to 7.23075, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 791ms/step - loss: 7.2307 - accuracy: 0.0054 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1880 - accuracy: 0.0060\n",
      "Epoch 27: loss improved from 7.23075 to 7.18800, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 828ms/step - loss: 7.1880 - accuracy: 0.0060 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1493 - accuracy: 0.0067\n",
      "Epoch 28: loss improved from 7.18800 to 7.14930, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 819ms/step - loss: 7.1493 - accuracy: 0.0067 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1175 - accuracy: 0.0060\n",
      "Epoch 29: loss improved from 7.14930 to 7.11751, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 777ms/step - loss: 7.1175 - accuracy: 0.0060 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0750 - accuracy: 0.0062\n",
      "Epoch 30: loss improved from 7.11751 to 7.07503, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 783ms/step - loss: 7.0750 - accuracy: 0.0062 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0428 - accuracy: 0.0081\n",
      "Epoch 31: loss improved from 7.07503 to 7.04280, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 788ms/step - loss: 7.0428 - accuracy: 0.0081 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0132 - accuracy: 0.0066\n",
      "Epoch 32: loss improved from 7.04280 to 7.01320, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 798ms/step - loss: 7.0132 - accuracy: 0.0066 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9790 - accuracy: 0.0069\n",
      "Epoch 33: loss improved from 7.01320 to 6.97896, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 818ms/step - loss: 6.9790 - accuracy: 0.0069 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9576 - accuracy: 0.0088\n",
      "Epoch 34: loss improved from 6.97896 to 6.95763, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 804ms/step - loss: 6.9576 - accuracy: 0.0088 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9250 - accuracy: 0.0082\n",
      "Epoch 35: loss improved from 6.95763 to 6.92504, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 802ms/step - loss: 6.9250 - accuracy: 0.0082 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9019 - accuracy: 0.0087\n",
      "Epoch 36: loss improved from 6.92504 to 6.90195, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 748ms/step - loss: 6.9019 - accuracy: 0.0087 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8733 - accuracy: 0.0085\n",
      "Epoch 37: loss improved from 6.90195 to 6.87334, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 764ms/step - loss: 6.8733 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8602 - accuracy: 0.0091\n",
      "Epoch 38: loss improved from 6.87334 to 6.86021, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 805ms/step - loss: 6.8602 - accuracy: 0.0091 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8367 - accuracy: 0.0085\n",
      "Epoch 39: loss improved from 6.86021 to 6.83674, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 825ms/step - loss: 6.8367 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8120 - accuracy: 0.0095\n",
      "Epoch 40: loss improved from 6.83674 to 6.81197, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 786ms/step - loss: 6.8120 - accuracy: 0.0095 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7929 - accuracy: 0.0092\n",
      "Epoch 41: loss improved from 6.81197 to 6.79292, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 811ms/step - loss: 6.7929 - accuracy: 0.0092 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7805 - accuracy: 0.0094\n",
      "Epoch 42: loss improved from 6.79292 to 6.78048, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 807ms/step - loss: 6.7805 - accuracy: 0.0094 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7600 - accuracy: 0.0087\n",
      "Epoch 43: loss improved from 6.78048 to 6.76000, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 796ms/step - loss: 6.7600 - accuracy: 0.0087 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7504 - accuracy: 0.0081\n",
      "Epoch 44: loss improved from 6.76000 to 6.75044, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 756ms/step - loss: 6.7504 - accuracy: 0.0081 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7301 - accuracy: 0.0100\n",
      "Epoch 45: loss improved from 6.75044 to 6.73012, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 784ms/step - loss: 6.7301 - accuracy: 0.0100 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7129 - accuracy: 0.0090\n",
      "Epoch 46: loss improved from 6.73012 to 6.71293, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 800ms/step - loss: 6.7129 - accuracy: 0.0090 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6920 - accuracy: 0.0088\n",
      "Epoch 47: loss improved from 6.71293 to 6.69204, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 800ms/step - loss: 6.6920 - accuracy: 0.0088 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6647 - accuracy: 0.0112\n",
      "Epoch 48: loss improved from 6.69204 to 6.66471, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 811ms/step - loss: 6.6647 - accuracy: 0.0112 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6528 - accuracy: 0.0090\n",
      "Epoch 49: loss improved from 6.66471 to 6.65281, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 781ms/step - loss: 6.6528 - accuracy: 0.0090 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6367 - accuracy: 0.0104\n",
      "Epoch 50: loss improved from 6.65281 to 6.63668, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 785ms/step - loss: 6.6367 - accuracy: 0.0104 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6162 - accuracy: 0.0107\n",
      "Epoch 51: loss improved from 6.63668 to 6.61622, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 785ms/step - loss: 6.6162 - accuracy: 0.0107 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6139 - accuracy: 0.0092\n",
      "Epoch 52: loss improved from 6.61622 to 6.61391, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 787ms/step - loss: 6.6139 - accuracy: 0.0092 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5965 - accuracy: 0.0096\n",
      "Epoch 53: loss improved from 6.61391 to 6.59651, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 797ms/step - loss: 6.5965 - accuracy: 0.0096 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5810 - accuracy: 0.0105\n",
      "Epoch 54: loss improved from 6.59651 to 6.58095, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 771ms/step - loss: 6.5810 - accuracy: 0.0105 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5758 - accuracy: 0.0094\n",
      "Epoch 55: loss improved from 6.58095 to 6.57582, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 794ms/step - loss: 6.5758 - accuracy: 0.0094 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5609 - accuracy: 0.0105\n",
      "Epoch 56: loss improved from 6.57582 to 6.56087, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 792ms/step - loss: 6.5609 - accuracy: 0.0105 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5457 - accuracy: 0.0115\n",
      "Epoch 57: loss improved from 6.56087 to 6.54569, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 784ms/step - loss: 6.5457 - accuracy: 0.0115 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5275 - accuracy: 0.0107\n",
      "Epoch 58: loss improved from 6.54569 to 6.52748, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 817ms/step - loss: 6.5275 - accuracy: 0.0107 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5115 - accuracy: 0.0126\n",
      "Epoch 59: loss improved from 6.52748 to 6.51153, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 796ms/step - loss: 6.5115 - accuracy: 0.0126 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5107 - accuracy: 0.0097\n",
      "Epoch 60: loss improved from 6.51153 to 6.51072, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 806ms/step - loss: 6.5107 - accuracy: 0.0097 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5022 - accuracy: 0.0131\n",
      "Epoch 61: loss improved from 6.51072 to 6.50217, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 772ms/step - loss: 6.5022 - accuracy: 0.0131 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4860 - accuracy: 0.0102\n",
      "Epoch 62: loss improved from 6.50217 to 6.48602, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 759ms/step - loss: 6.4860 - accuracy: 0.0102 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4732 - accuracy: 0.0119\n",
      "Epoch 63: loss improved from 6.48602 to 6.47319, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 798ms/step - loss: 6.4732 - accuracy: 0.0119 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4546 - accuracy: 0.0115\n",
      "Epoch 64: loss improved from 6.47319 to 6.45461, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 788ms/step - loss: 6.4546 - accuracy: 0.0115 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4440 - accuracy: 0.0115\n",
      "Epoch 65: loss improved from 6.45461 to 6.44405, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 803ms/step - loss: 6.4440 - accuracy: 0.0115 - lr: 0.0010\n",
      "Epoch 66/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4285 - accuracy: 0.0104\n",
      "Epoch 66: loss improved from 6.44405 to 6.42847, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 786ms/step - loss: 6.4285 - accuracy: 0.0104 - lr: 0.0010\n",
      "Epoch 67/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4155 - accuracy: 0.0122\n",
      "Epoch 67: loss improved from 6.42847 to 6.41554, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 794ms/step - loss: 6.4155 - accuracy: 0.0122 - lr: 0.0010\n",
      "Epoch 68/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4047 - accuracy: 0.0132\n",
      "Epoch 68: loss improved from 6.41554 to 6.40471, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 814ms/step - loss: 6.4047 - accuracy: 0.0132 - lr: 0.0010\n",
      "Epoch 69/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3943 - accuracy: 0.0121\n",
      "Epoch 69: loss improved from 6.40471 to 6.39425, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 811ms/step - loss: 6.3943 - accuracy: 0.0121 - lr: 0.0010\n",
      "Epoch 70/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3739 - accuracy: 0.0127\n",
      "Epoch 70: loss improved from 6.39425 to 6.37388, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 811ms/step - loss: 6.3739 - accuracy: 0.0127 - lr: 0.0010\n",
      "Epoch 71/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3736 - accuracy: 0.0137\n",
      "Epoch 71: loss improved from 6.37388 to 6.37357, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 759ms/step - loss: 6.3736 - accuracy: 0.0137 - lr: 0.0010\n",
      "Epoch 72/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3568 - accuracy: 0.0138\n",
      "Epoch 72: loss improved from 6.37357 to 6.35680, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 762ms/step - loss: 6.3568 - accuracy: 0.0138 - lr: 0.0010\n",
      "Epoch 73/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3437 - accuracy: 0.0136\n",
      "Epoch 73: loss improved from 6.35680 to 6.34370, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 798ms/step - loss: 6.3437 - accuracy: 0.0136 - lr: 0.0010\n",
      "Epoch 74/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3426 - accuracy: 0.0134\n",
      "Epoch 74: loss improved from 6.34370 to 6.34256, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 821ms/step - loss: 6.3426 - accuracy: 0.0134 - lr: 0.0010\n",
      "Epoch 75/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3283 - accuracy: 0.0142\n",
      "Epoch 75: loss improved from 6.34256 to 6.32829, saving model to nextword1.h5\n",
      "15/15 [==============================] - 14s 921ms/step - loss: 6.3283 - accuracy: 0.0142 - lr: 0.0010\n",
      "Epoch 76/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3288 - accuracy: 0.0142\n",
      "Epoch 76: loss did not improve from 6.32829\n",
      "15/15 [==============================] - 60s 4s/step - loss: 6.3288 - accuracy: 0.0142 - lr: 0.0010\n",
      "Epoch 77/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3138 - accuracy: 0.0142\n",
      "Epoch 77: loss improved from 6.32829 to 6.31377, saving model to nextword1.h5\n",
      "15/15 [==============================] - 25s 2s/step - loss: 6.3138 - accuracy: 0.0142 - lr: 0.0010\n",
      "Epoch 78/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2906 - accuracy: 0.0134\n",
      "Epoch 78: loss improved from 6.31377 to 6.29062, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 727ms/step - loss: 6.2906 - accuracy: 0.0134 - lr: 0.0010\n",
      "Epoch 79/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2760 - accuracy: 0.0147\n",
      "Epoch 79: loss improved from 6.29062 to 6.27599, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 825ms/step - loss: 6.2760 - accuracy: 0.0147 - lr: 0.0010\n",
      "Epoch 80/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2617 - accuracy: 0.0144 \n",
      "Epoch 80: loss improved from 6.27599 to 6.26170, saving model to nextword1.h5\n",
      "15/15 [==============================] - 268s 19s/step - loss: 6.2617 - accuracy: 0.0144 - lr: 0.0010\n",
      "Epoch 81/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2397 - accuracy: 0.0151\n",
      "Epoch 81: loss improved from 6.26170 to 6.23966, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 804ms/step - loss: 6.2397 - accuracy: 0.0151 - lr: 0.0010\n",
      "Epoch 82/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2253 - accuracy: 0.0147\n",
      "Epoch 82: loss improved from 6.23966 to 6.22527, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 749ms/step - loss: 6.2253 - accuracy: 0.0147 - lr: 0.0010\n",
      "Epoch 83/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2024 - accuracy: 0.0168\n",
      "Epoch 83: loss improved from 6.22527 to 6.20236, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 787ms/step - loss: 6.2024 - accuracy: 0.0168 - lr: 0.0010\n",
      "Epoch 84/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1801 - accuracy: 0.0158\n",
      "Epoch 84: loss improved from 6.20236 to 6.18014, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 776ms/step - loss: 6.1801 - accuracy: 0.0158 - lr: 0.0010\n",
      "Epoch 85/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1557 - accuracy: 0.0166\n",
      "Epoch 85: loss improved from 6.18014 to 6.15573, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 738ms/step - loss: 6.1557 - accuracy: 0.0166 - lr: 0.0010\n",
      "Epoch 86/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1062 - accuracy: 0.0171\n",
      "Epoch 86: loss improved from 6.15573 to 6.10624, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 756ms/step - loss: 6.1062 - accuracy: 0.0171 - lr: 0.0010\n",
      "Epoch 87/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0714 - accuracy: 0.0174\n",
      "Epoch 87: loss improved from 6.10624 to 6.07142, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 791ms/step - loss: 6.0714 - accuracy: 0.0174 - lr: 0.0010\n",
      "Epoch 88/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0210 - accuracy: 0.0171\n",
      "Epoch 88: loss improved from 6.07142 to 6.02104, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 773ms/step - loss: 6.0210 - accuracy: 0.0171 - lr: 0.0010\n",
      "Epoch 89/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9817 - accuracy: 0.0191\n",
      "Epoch 89: loss improved from 6.02104 to 5.98168, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 787ms/step - loss: 5.9817 - accuracy: 0.0191 - lr: 0.0010\n",
      "Epoch 90/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9252 - accuracy: 0.0190\n",
      "Epoch 90: loss improved from 5.98168 to 5.92515, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 772ms/step - loss: 5.9252 - accuracy: 0.0190 - lr: 0.0010\n",
      "Epoch 91/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.8557 - accuracy: 0.0218\n",
      "Epoch 91: loss improved from 5.92515 to 5.85569, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 775ms/step - loss: 5.8557 - accuracy: 0.0218 - lr: 0.0010\n",
      "Epoch 92/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.7742 - accuracy: 0.0245\n",
      "Epoch 92: loss improved from 5.85569 to 5.77421, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 793ms/step - loss: 5.7742 - accuracy: 0.0245 - lr: 0.0010\n",
      "Epoch 93/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.6906 - accuracy: 0.0267\n",
      "Epoch 93: loss improved from 5.77421 to 5.69063, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 843ms/step - loss: 5.6906 - accuracy: 0.0267 - lr: 0.0010\n",
      "Epoch 94/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.6173 - accuracy: 0.0246\n",
      "Epoch 94: loss improved from 5.69063 to 5.61734, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 823ms/step - loss: 5.6173 - accuracy: 0.0246 - lr: 0.0010\n",
      "Epoch 95/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.5361 - accuracy: 0.0278\n",
      "Epoch 95: loss improved from 5.61734 to 5.53611, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 894ms/step - loss: 5.5361 - accuracy: 0.0278 - lr: 0.0010\n",
      "Epoch 96/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.4663 - accuracy: 0.0302\n",
      "Epoch 96: loss improved from 5.53611 to 5.46635, saving model to nextword1.h5\n",
      "15/15 [==============================] - 14s 900ms/step - loss: 5.4663 - accuracy: 0.0302 - lr: 0.0010\n",
      "Epoch 97/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.4047 - accuracy: 0.0317\n",
      "Epoch 97: loss improved from 5.46635 to 5.40471, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 853ms/step - loss: 5.4047 - accuracy: 0.0317 - lr: 0.0010\n",
      "Epoch 98/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.3351 - accuracy: 0.0330\n",
      "Epoch 98: loss improved from 5.40471 to 5.33514, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 800ms/step - loss: 5.3351 - accuracy: 0.0330 - lr: 0.0010\n",
      "Epoch 99/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.2812 - accuracy: 0.0346\n",
      "Epoch 99: loss improved from 5.33514 to 5.28124, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 800ms/step - loss: 5.2812 - accuracy: 0.0346 - lr: 0.0010\n",
      "Epoch 100/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.2195 - accuracy: 0.0366\n",
      "Epoch 100: loss improved from 5.28124 to 5.21954, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 799ms/step - loss: 5.2195 - accuracy: 0.0366 - lr: 0.0010\n",
      "Epoch 101/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.1842 - accuracy: 0.0388\n",
      "Epoch 101: loss improved from 5.21954 to 5.18423, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 817ms/step - loss: 5.1842 - accuracy: 0.0388 - lr: 0.0010\n",
      "Epoch 102/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.1124 - accuracy: 0.0418\n",
      "Epoch 102: loss improved from 5.18423 to 5.11235, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 834ms/step - loss: 5.1124 - accuracy: 0.0418 - lr: 0.0010\n",
      "Epoch 103/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0751 - accuracy: 0.0443\n",
      "Epoch 103: loss improved from 5.11235 to 5.07510, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 801ms/step - loss: 5.0751 - accuracy: 0.0443 - lr: 0.0010\n",
      "Epoch 104/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0185 - accuracy: 0.0468\n",
      "Epoch 104: loss improved from 5.07510 to 5.01848, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 822ms/step - loss: 5.0185 - accuracy: 0.0468 - lr: 0.0010\n",
      "Epoch 105/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9652 - accuracy: 0.0489\n",
      "Epoch 105: loss improved from 5.01848 to 4.96525, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 793ms/step - loss: 4.9652 - accuracy: 0.0489 - lr: 0.0010\n",
      "Epoch 106/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9306 - accuracy: 0.0501\n",
      "Epoch 106: loss improved from 4.96525 to 4.93060, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 813ms/step - loss: 4.9306 - accuracy: 0.0501 - lr: 0.0010\n",
      "Epoch 107/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9016 - accuracy: 0.0514\n",
      "Epoch 107: loss improved from 4.93060 to 4.90164, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 801ms/step - loss: 4.9016 - accuracy: 0.0514 - lr: 0.0010\n",
      "Epoch 108/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8647 - accuracy: 0.0537\n",
      "Epoch 108: loss improved from 4.90164 to 4.86472, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 815ms/step - loss: 4.8647 - accuracy: 0.0537 - lr: 0.0010\n",
      "Epoch 109/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8213 - accuracy: 0.0571\n",
      "Epoch 109: loss improved from 4.86472 to 4.82134, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 819ms/step - loss: 4.8213 - accuracy: 0.0571 - lr: 0.0010\n",
      "Epoch 110/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7761 - accuracy: 0.0580\n",
      "Epoch 110: loss improved from 4.82134 to 4.77609, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 814ms/step - loss: 4.7761 - accuracy: 0.0580 - lr: 0.0010\n",
      "Epoch 111/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7294 - accuracy: 0.0592\n",
      "Epoch 111: loss improved from 4.77609 to 4.72945, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 816ms/step - loss: 4.7294 - accuracy: 0.0592 - lr: 0.0010\n",
      "Epoch 112/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6921 - accuracy: 0.0599\n",
      "Epoch 112: loss improved from 4.72945 to 4.69214, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 803ms/step - loss: 4.6921 - accuracy: 0.0599 - lr: 0.0010\n",
      "Epoch 113/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6771 - accuracy: 0.0630\n",
      "Epoch 113: loss improved from 4.69214 to 4.67708, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 787ms/step - loss: 4.6771 - accuracy: 0.0630 - lr: 0.0010\n",
      "Epoch 114/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6446 - accuracy: 0.0633\n",
      "Epoch 114: loss improved from 4.67708 to 4.64460, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 806ms/step - loss: 4.6446 - accuracy: 0.0633 - lr: 0.0010\n",
      "Epoch 115/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6069 - accuracy: 0.0672\n",
      "Epoch 115: loss improved from 4.64460 to 4.60688, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 787ms/step - loss: 4.6069 - accuracy: 0.0672 - lr: 0.0010\n",
      "Epoch 116/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5911 - accuracy: 0.0636\n",
      "Epoch 116: loss improved from 4.60688 to 4.59106, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 823ms/step - loss: 4.5911 - accuracy: 0.0636 - lr: 0.0010\n",
      "Epoch 117/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5519 - accuracy: 0.0682\n",
      "Epoch 117: loss improved from 4.59106 to 4.55195, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 807ms/step - loss: 4.5519 - accuracy: 0.0682 - lr: 0.0010\n",
      "Epoch 118/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5195 - accuracy: 0.0683\n",
      "Epoch 118: loss improved from 4.55195 to 4.51952, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 784ms/step - loss: 4.5195 - accuracy: 0.0683 - lr: 0.0010\n",
      "Epoch 119/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4969 - accuracy: 0.0693\n",
      "Epoch 119: loss improved from 4.51952 to 4.49691, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 856ms/step - loss: 4.4969 - accuracy: 0.0693 - lr: 0.0010\n",
      "Epoch 120/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4859 - accuracy: 0.0709\n",
      "Epoch 120: loss improved from 4.49691 to 4.48592, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 811ms/step - loss: 4.4859 - accuracy: 0.0709 - lr: 0.0010\n",
      "Epoch 121/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4539 - accuracy: 0.0738\n",
      "Epoch 121: loss improved from 4.48592 to 4.45391, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 821ms/step - loss: 4.4539 - accuracy: 0.0738 - lr: 0.0010\n",
      "Epoch 122/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4307 - accuracy: 0.0761\n",
      "Epoch 122: loss improved from 4.45391 to 4.43071, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 806ms/step - loss: 4.4307 - accuracy: 0.0761 - lr: 0.0010\n",
      "Epoch 123/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4142 - accuracy: 0.0725\n",
      "Epoch 123: loss improved from 4.43071 to 4.41423, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 805ms/step - loss: 4.4142 - accuracy: 0.0725 - lr: 0.0010\n",
      "Epoch 124/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3813 - accuracy: 0.0754\n",
      "Epoch 124: loss improved from 4.41423 to 4.38129, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 821ms/step - loss: 4.3813 - accuracy: 0.0754 - lr: 0.0010\n",
      "Epoch 125/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3300 - accuracy: 0.0799\n",
      "Epoch 125: loss improved from 4.38129 to 4.32997, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 792ms/step - loss: 4.3300 - accuracy: 0.0799 - lr: 0.0010\n",
      "Epoch 126/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3125 - accuracy: 0.0826\n",
      "Epoch 126: loss improved from 4.32997 to 4.31253, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 818ms/step - loss: 4.3125 - accuracy: 0.0826 - lr: 0.0010\n",
      "Epoch 127/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2763 - accuracy: 0.0868\n",
      "Epoch 127: loss improved from 4.31253 to 4.27629, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 798ms/step - loss: 4.2763 - accuracy: 0.0868 - lr: 0.0010\n",
      "Epoch 128/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2528 - accuracy: 0.0886\n",
      "Epoch 128: loss improved from 4.27629 to 4.25283, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 810ms/step - loss: 4.2528 - accuracy: 0.0886 - lr: 0.0010\n",
      "Epoch 129/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2145 - accuracy: 0.0902\n",
      "Epoch 129: loss improved from 4.25283 to 4.21454, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 813ms/step - loss: 4.2145 - accuracy: 0.0902 - lr: 0.0010\n",
      "Epoch 130/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1963 - accuracy: 0.0886\n",
      "Epoch 130: loss improved from 4.21454 to 4.19629, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 799ms/step - loss: 4.1963 - accuracy: 0.0886 - lr: 0.0010\n",
      "Epoch 131/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1665 - accuracy: 0.0926\n",
      "Epoch 131: loss improved from 4.19629 to 4.16649, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 803ms/step - loss: 4.1665 - accuracy: 0.0926 - lr: 0.0010\n",
      "Epoch 132/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1557 - accuracy: 0.0962\n",
      "Epoch 132: loss improved from 4.16649 to 4.15565, saving model to nextword1.h5\n",
      "15/15 [==============================] - 13s 860ms/step - loss: 4.1557 - accuracy: 0.0962 - lr: 0.0010\n",
      "Epoch 133/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1228 - accuracy: 0.0949 \n",
      "Epoch 133: loss improved from 4.15565 to 4.12283, saving model to nextword1.h5\n",
      "15/15 [==============================] - 310s 22s/step - loss: 4.1228 - accuracy: 0.0949 - lr: 0.0010\n",
      "Epoch 134/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1128 - accuracy: 0.0961\n",
      "Epoch 134: loss improved from 4.12283 to 4.11285, saving model to nextword1.h5\n",
      "15/15 [==============================] - 14s 965ms/step - loss: 4.1128 - accuracy: 0.0961 - lr: 0.0010\n",
      "Epoch 135/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0778 - accuracy: 0.1002\n",
      "Epoch 135: loss improved from 4.11285 to 4.07783, saving model to nextword1.h5\n",
      "15/15 [==============================] - 27s 2s/step - loss: 4.0778 - accuracy: 0.1002 - lr: 0.0010\n",
      "Epoch 136/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0644 - accuracy: 0.1014 \n",
      "Epoch 136: loss improved from 4.07783 to 4.06442, saving model to nextword1.h5\n",
      "15/15 [==============================] - 397s 28s/step - loss: 4.0644 - accuracy: 0.1014 - lr: 0.0010\n",
      "Epoch 137/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0227 - accuracy: 0.1050\n",
      "Epoch 137: loss improved from 4.06442 to 4.02275, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 781ms/step - loss: 4.0227 - accuracy: 0.1050 - lr: 0.0010\n",
      "Epoch 138/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9852 - accuracy: 0.1044\n",
      "Epoch 138: loss improved from 4.02275 to 3.98516, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 733ms/step - loss: 3.9852 - accuracy: 0.1044 - lr: 0.0010\n",
      "Epoch 139/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9507 - accuracy: 0.1098\n",
      "Epoch 139: loss improved from 3.98516 to 3.95066, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 761ms/step - loss: 3.9507 - accuracy: 0.1098 - lr: 0.0010\n",
      "Epoch 140/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9021 - accuracy: 0.1126\n",
      "Epoch 140: loss improved from 3.95066 to 3.90210, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 751ms/step - loss: 3.9021 - accuracy: 0.1126 - lr: 0.0010\n",
      "Epoch 141/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.8547 - accuracy: 0.1190\n",
      "Epoch 141: loss improved from 3.90210 to 3.85465, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 758ms/step - loss: 3.8547 - accuracy: 0.1190 - lr: 0.0010\n",
      "Epoch 142/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.8133 - accuracy: 0.1236\n",
      "Epoch 142: loss improved from 3.85465 to 3.81328, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 783ms/step - loss: 3.8133 - accuracy: 0.1236 - lr: 0.0010\n",
      "Epoch 143/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.7618 - accuracy: 0.1289\n",
      "Epoch 143: loss improved from 3.81328 to 3.76183, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 746ms/step - loss: 3.7618 - accuracy: 0.1289 - lr: 0.0010\n",
      "Epoch 144/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.6962 - accuracy: 0.1331\n",
      "Epoch 144: loss improved from 3.76183 to 3.69615, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 813ms/step - loss: 3.6962 - accuracy: 0.1331 - lr: 0.0010\n",
      "Epoch 145/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.6513 - accuracy: 0.1397\n",
      "Epoch 145: loss improved from 3.69615 to 3.65129, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 772ms/step - loss: 3.6513 - accuracy: 0.1397 - lr: 0.0010\n",
      "Epoch 146/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.6096 - accuracy: 0.1468\n",
      "Epoch 146: loss improved from 3.65129 to 3.60956, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 762ms/step - loss: 3.6096 - accuracy: 0.1468 - lr: 0.0010\n",
      "Epoch 147/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.5705 - accuracy: 0.1480\n",
      "Epoch 147: loss improved from 3.60956 to 3.57054, saving model to nextword1.h5\n",
      "15/15 [==============================] - 12s 788ms/step - loss: 3.5705 - accuracy: 0.1480 - lr: 0.0010\n",
      "Epoch 148/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.5311 - accuracy: 0.1594\n",
      "Epoch 148: loss improved from 3.57054 to 3.53108, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 744ms/step - loss: 3.5311 - accuracy: 0.1594 - lr: 0.0010\n",
      "Epoch 149/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.4978 - accuracy: 0.1609\n",
      "Epoch 149: loss improved from 3.53108 to 3.49782, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 762ms/step - loss: 3.4978 - accuracy: 0.1609 - lr: 0.0010\n",
      "Epoch 150/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.4329 - accuracy: 0.1680\n",
      "Epoch 150: loss improved from 3.49782 to 3.43294, saving model to nextword1.h5\n",
      "15/15 [==============================] - 11s 768ms/step - loss: 3.4329 - accuracy: 0.1680 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fadd692e550>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=1000, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"nextword.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"nextword.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('nextword.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"nextword.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f747f4033574bc018cd224c8390eda22f5cf19741325755a1e4dd992a19a8100"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
