{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data/praire-clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened my eyes and saw a pea green world all around me  Then I heard the doctor say   Give  er another whiff or two   His voice sounded far away  as though he were speaking through the Simplon Tunnel  and not merely through his teeth  within twelve inches of my nose   I took my whiff or two  I gulped at that chloroform like a thirsty Bedouin at a wadi spring  I went down into the pea green emptiness again  and forgot about the Kelly pad and the recurring waves of pain that came bigger and bigg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I opened my eyes and saw a pea-green world all around me. Then heard the doctor say: \"Give \\'er another whiff or two.\" His voice sounded far-away, as though he were speaking through Simplon Tunnel, not merely his teeth, within twelve inches of nose. took two. gulped at that chloroform like thirsty Bedouin wadi-spring. went down into emptiness again, forgot about Kelly pad recurring waves pain came bigger tried to sweep racked old body breakers ribs stranded schooner. hateful metallic clink steel '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31, 2618, 403, 404, 1, 657, 24, 2619, 87, 88]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('../model/tokenizer_avi.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8595\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [  31 2618  403  404    1]\n",
      "The responses are:  [2618  403  404    1  657]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1, 10)             85950     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8595)              8603595   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,738,545\n",
      "Trainable params: 21,738,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"../model/nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),metrics=[perplexity,\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 04:46:31.827786: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:46:32.214841: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:46:32.261788: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:46:32.501529: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 04:46:32.591810: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - ETA: 0s - loss: 9.0588 - perplexity: 533.3113 - accuracy: 0.0026\n",
      "Epoch 1: loss improved from inf to 9.05880, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 5s 197ms/step - loss: 9.0588 - perplexity: 533.3113 - accuracy: 0.0026 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 9.0209 - perplexity: 527.5516 - accuracy: 0.0035\n",
      "Epoch 2: loss improved from 9.05880 to 9.02090, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 9.0209 - perplexity: 527.5516 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.8877 - perplexity: 547.2454 - accuracy: 0.0035\n",
      "Epoch 3: loss improved from 9.02090 to 8.88768, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 8.8877 - perplexity: 547.2454 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.8319 - perplexity: 513.5602 - accuracy: 0.0035\n",
      "Epoch 4: loss improved from 8.88768 to 8.83193, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 8.8319 - perplexity: 513.5602 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.7052 - perplexity: 497.9892 - accuracy: 0.0035\n",
      "Epoch 5: loss improved from 8.83193 to 8.70516, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 183ms/step - loss: 8.7052 - perplexity: 497.9892 - accuracy: 0.0035 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.5657 - perplexity: 471.5068 - accuracy: 0.0032\n",
      "Epoch 6: loss improved from 8.70516 to 8.56570, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 8.5657 - perplexity: 471.5068 - accuracy: 0.0032 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.4522 - perplexity: 437.5071 - accuracy: 0.0031\n",
      "Epoch 7: loss improved from 8.56570 to 8.45221, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 8.4522 - perplexity: 437.5071 - accuracy: 0.0031 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.3406 - perplexity: 410.7385 - accuracy: 0.0040\n",
      "Epoch 8: loss improved from 8.45221 to 8.34061, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 8.3406 - perplexity: 410.7385 - accuracy: 0.0040 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.2581 - perplexity: 377.9696 - accuracy: 0.0027\n",
      "Epoch 9: loss improved from 8.34061 to 8.25811, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 8.2581 - perplexity: 377.9696 - accuracy: 0.0027 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.1878 - perplexity: 355.0096 - accuracy: 0.0029\n",
      "Epoch 10: loss improved from 8.25811 to 8.18775, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 8.1878 - perplexity: 355.0096 - accuracy: 0.0029 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.1296 - perplexity: 339.1578 - accuracy: 0.0033\n",
      "Epoch 11: loss improved from 8.18775 to 8.12962, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 8.1296 - perplexity: 339.1578 - accuracy: 0.0033 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0928 - perplexity: 329.6555 - accuracy: 0.0022\n",
      "Epoch 12: loss improved from 8.12962 to 8.09279, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 8.0928 - perplexity: 329.6555 - accuracy: 0.0022 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0535 - perplexity: 318.9613 - accuracy: 0.0037\n",
      "Epoch 13: loss improved from 8.09279 to 8.05352, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 8.0535 - perplexity: 318.9613 - accuracy: 0.0037 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 8.0135 - perplexity: 309.2832 - accuracy: 0.0028\n",
      "Epoch 14: loss improved from 8.05352 to 8.01347, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 8.0135 - perplexity: 309.2832 - accuracy: 0.0028 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.9654 - perplexity: 298.5190 - accuracy: 0.0040\n",
      "Epoch 15: loss improved from 8.01347 to 7.96540, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 7.9654 - perplexity: 298.5190 - accuracy: 0.0040 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.8999 - perplexity: 285.6448 - accuracy: 0.0043\n",
      "Epoch 16: loss improved from 7.96540 to 7.89995, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 7.8999 - perplexity: 285.6448 - accuracy: 0.0043 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.8178 - perplexity: 270.6948 - accuracy: 0.0041\n",
      "Epoch 17: loss improved from 7.89995 to 7.81782, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 7.8178 - perplexity: 270.6948 - accuracy: 0.0041 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.7273 - perplexity: 254.5979 - accuracy: 0.0045\n",
      "Epoch 18: loss improved from 7.81782 to 7.72730, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 7.7273 - perplexity: 254.5979 - accuracy: 0.0045 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.6289 - perplexity: 237.4203 - accuracy: 0.0036\n",
      "Epoch 19: loss improved from 7.72730 to 7.62886, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 7.6289 - perplexity: 237.4203 - accuracy: 0.0036 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.5352 - perplexity: 224.9404 - accuracy: 0.0056\n",
      "Epoch 20: loss improved from 7.62886 to 7.53525, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 7.5352 - perplexity: 224.9404 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.4610 - perplexity: 212.0109 - accuracy: 0.0058\n",
      "Epoch 21: loss improved from 7.53525 to 7.46096, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 7.4610 - perplexity: 212.0109 - accuracy: 0.0058 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.4102 - perplexity: 205.6198 - accuracy: 0.0062\n",
      "Epoch 22: loss improved from 7.46096 to 7.41016, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 7.4102 - perplexity: 205.6198 - accuracy: 0.0062 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.3562 - perplexity: 198.0181 - accuracy: 0.0052\n",
      "Epoch 23: loss improved from 7.41016 to 7.35621, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 7.3562 - perplexity: 198.0181 - accuracy: 0.0052 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.3069 - perplexity: 192.4074 - accuracy: 0.0066\n",
      "Epoch 24: loss improved from 7.35621 to 7.30686, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 7.3069 - perplexity: 192.4074 - accuracy: 0.0066 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.2563 - perplexity: 185.9362 - accuracy: 0.0062\n",
      "Epoch 25: loss improved from 7.30686 to 7.25626, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 7.2563 - perplexity: 185.9362 - accuracy: 0.0062 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.2004 - perplexity: 177.8811 - accuracy: 0.0066\n",
      "Epoch 26: loss improved from 7.25626 to 7.20039, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 7.2004 - perplexity: 177.8811 - accuracy: 0.0066 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1494 - perplexity: 172.8978 - accuracy: 0.0065\n",
      "Epoch 27: loss improved from 7.20039 to 7.14937, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 7.1494 - perplexity: 172.8978 - accuracy: 0.0065 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.1194 - perplexity: 169.1311 - accuracy: 0.0071\n",
      "Epoch 28: loss improved from 7.14937 to 7.11944, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 7.1194 - perplexity: 169.1311 - accuracy: 0.0071 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0833 - perplexity: 166.1040 - accuracy: 0.0072\n",
      "Epoch 29: loss improved from 7.11944 to 7.08329, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 7.0833 - perplexity: 166.1040 - accuracy: 0.0072 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0440 - perplexity: 160.5094 - accuracy: 0.0079\n",
      "Epoch 30: loss improved from 7.08329 to 7.04402, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 7.0440 - perplexity: 160.5094 - accuracy: 0.0079 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 7.0111 - perplexity: 157.9322 - accuracy: 0.0068\n",
      "Epoch 31: loss improved from 7.04402 to 7.01109, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 7.0111 - perplexity: 157.9322 - accuracy: 0.0068 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9873 - perplexity: 155.3763 - accuracy: 0.0066\n",
      "Epoch 32: loss improved from 7.01109 to 6.98733, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 6.9873 - perplexity: 155.3763 - accuracy: 0.0066 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9610 - perplexity: 152.6692 - accuracy: 0.0071\n",
      "Epoch 33: loss improved from 6.98733 to 6.96096, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 6.9610 - perplexity: 152.6692 - accuracy: 0.0071 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9361 - perplexity: 149.4272 - accuracy: 0.0073\n",
      "Epoch 34: loss improved from 6.96096 to 6.93611, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 6.9361 - perplexity: 149.4272 - accuracy: 0.0073 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.9059 - perplexity: 147.0159 - accuracy: 0.0077\n",
      "Epoch 35: loss improved from 6.93611 to 6.90589, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.9059 - perplexity: 147.0159 - accuracy: 0.0077 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8856 - perplexity: 145.2715 - accuracy: 0.0081\n",
      "Epoch 36: loss improved from 6.90589 to 6.88557, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 6.8856 - perplexity: 145.2715 - accuracy: 0.0081 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8591 - perplexity: 144.5536 - accuracy: 0.0085\n",
      "Epoch 37: loss improved from 6.88557 to 6.85907, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.8591 - perplexity: 144.5536 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8322 - perplexity: 140.3782 - accuracy: 0.0082\n",
      "Epoch 38: loss improved from 6.85907 to 6.83222, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.8322 - perplexity: 140.3782 - accuracy: 0.0082 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.8189 - perplexity: 139.1988 - accuracy: 0.0075\n",
      "Epoch 39: loss improved from 6.83222 to 6.81890, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.8189 - perplexity: 139.1988 - accuracy: 0.0075 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7967 - perplexity: 137.0108 - accuracy: 0.0083\n",
      "Epoch 40: loss improved from 6.81890 to 6.79669, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.7967 - perplexity: 137.0108 - accuracy: 0.0083 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7848 - perplexity: 136.3903 - accuracy: 0.0080\n",
      "Epoch 41: loss improved from 6.79669 to 6.78484, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.7848 - perplexity: 136.3903 - accuracy: 0.0080 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7597 - perplexity: 133.0930 - accuracy: 0.0083\n",
      "Epoch 42: loss improved from 6.78484 to 6.75971, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.7597 - perplexity: 133.0930 - accuracy: 0.0083 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7366 - perplexity: 131.7404 - accuracy: 0.0104\n",
      "Epoch 43: loss improved from 6.75971 to 6.73665, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.7366 - perplexity: 131.7404 - accuracy: 0.0104 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7160 - perplexity: 130.0534 - accuracy: 0.0094\n",
      "Epoch 44: loss improved from 6.73665 to 6.71597, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.7160 - perplexity: 130.0534 - accuracy: 0.0094 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.7058 - perplexity: 129.4981 - accuracy: 0.0094\n",
      "Epoch 45: loss improved from 6.71597 to 6.70578, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 6.7058 - perplexity: 129.4981 - accuracy: 0.0094 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6989 - perplexity: 132.5825 - accuracy: 0.0100\n",
      "Epoch 46: loss improved from 6.70578 to 6.69888, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 6.6989 - perplexity: 132.5825 - accuracy: 0.0100 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6694 - perplexity: 127.8316 - accuracy: 0.0098\n",
      "Epoch 47: loss improved from 6.69888 to 6.66942, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 6.6694 - perplexity: 127.8316 - accuracy: 0.0098 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6599 - perplexity: 126.0965 - accuracy: 0.0100\n",
      "Epoch 48: loss improved from 6.66942 to 6.65990, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.6599 - perplexity: 126.0965 - accuracy: 0.0100 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6553 - perplexity: 132.1804 - accuracy: 0.0103\n",
      "Epoch 49: loss improved from 6.65990 to 6.65532, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 6.6553 - perplexity: 132.1804 - accuracy: 0.0103 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6331 - perplexity: 125.9183 - accuracy: 0.0101\n",
      "Epoch 50: loss improved from 6.65532 to 6.63308, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.6331 - perplexity: 125.9183 - accuracy: 0.0101 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6169 - perplexity: 129.4981 - accuracy: 0.0098\n",
      "Epoch 51: loss improved from 6.63308 to 6.61694, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.6169 - perplexity: 129.4981 - accuracy: 0.0098 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.6013 - perplexity: 125.7686 - accuracy: 0.0108\n",
      "Epoch 52: loss improved from 6.61694 to 6.60128, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.6013 - perplexity: 125.7686 - accuracy: 0.0108 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5914 - perplexity: 125.5328 - accuracy: 0.0113\n",
      "Epoch 53: loss improved from 6.60128 to 6.59141, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.5914 - perplexity: 125.5328 - accuracy: 0.0113 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5711 - perplexity: 120.6611 - accuracy: 0.0107\n",
      "Epoch 54: loss improved from 6.59141 to 6.57110, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.5711 - perplexity: 120.6611 - accuracy: 0.0107 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5489 - perplexity: 118.6732 - accuracy: 0.0109\n",
      "Epoch 55: loss improved from 6.57110 to 6.54885, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.5489 - perplexity: 118.6732 - accuracy: 0.0109 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5371 - perplexity: 123.7799 - accuracy: 0.0102\n",
      "Epoch 56: loss improved from 6.54885 to 6.53707, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.5371 - perplexity: 123.7799 - accuracy: 0.0102 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5278 - perplexity: 123.2075 - accuracy: 0.0116\n",
      "Epoch 57: loss improved from 6.53707 to 6.52780, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.5278 - perplexity: 123.2075 - accuracy: 0.0116 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.5177 - perplexity: 116.4271 - accuracy: 0.0111\n",
      "Epoch 58: loss improved from 6.52780 to 6.51767, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.5177 - perplexity: 116.4271 - accuracy: 0.0111 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4985 - perplexity: 113.7779 - accuracy: 0.0099\n",
      "Epoch 59: loss improved from 6.51767 to 6.49850, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.4985 - perplexity: 113.7779 - accuracy: 0.0099 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4752 - perplexity: 120.4292 - accuracy: 0.0117\n",
      "Epoch 60: loss improved from 6.49850 to 6.47516, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.4752 - perplexity: 120.4292 - accuracy: 0.0117 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4515 - perplexity: 120.0286 - accuracy: 0.0141\n",
      "Epoch 61: loss improved from 6.47516 to 6.45152, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 6.4515 - perplexity: 120.0286 - accuracy: 0.0141 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4397 - perplexity: 117.5711 - accuracy: 0.0128\n",
      "Epoch 62: loss improved from 6.45152 to 6.43967, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.4397 - perplexity: 117.5711 - accuracy: 0.0128 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4282 - perplexity: 113.7437 - accuracy: 0.0123\n",
      "Epoch 63: loss improved from 6.43967 to 6.42819, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.4282 - perplexity: 113.7437 - accuracy: 0.0123 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4221 - perplexity: 118.5660 - accuracy: 0.0131\n",
      "Epoch 64: loss improved from 6.42819 to 6.42210, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.4221 - perplexity: 118.5660 - accuracy: 0.0131 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.4072 - perplexity: 328.1357 - accuracy: 0.0128\n",
      "Epoch 65: loss improved from 6.42210 to 6.40725, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 6.4072 - perplexity: 328.1357 - accuracy: 0.0128 - lr: 0.0010\n",
      "Epoch 66/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3994 - perplexity: 109.7384 - accuracy: 0.0137\n",
      "Epoch 66: loss improved from 6.40725 to 6.39935, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 6.3994 - perplexity: 109.7384 - accuracy: 0.0137 - lr: 0.0010\n",
      "Epoch 67/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3919 - perplexity: 113.9563 - accuracy: 0.0126\n",
      "Epoch 67: loss improved from 6.39935 to 6.39193, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.3919 - perplexity: 113.9563 - accuracy: 0.0126 - lr: 0.0010\n",
      "Epoch 68/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3691 - perplexity: 113.8132 - accuracy: 0.0135\n",
      "Epoch 68: loss improved from 6.39193 to 6.36908, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.3691 - perplexity: 113.8132 - accuracy: 0.0135 - lr: 0.0010\n",
      "Epoch 69/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3379 - perplexity: 116.6678 - accuracy: 0.0137\n",
      "Epoch 69: loss improved from 6.36908 to 6.33795, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.3379 - perplexity: 116.6678 - accuracy: 0.0137 - lr: 0.0010\n",
      "Epoch 70/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.3031 - perplexity: 105.8391 - accuracy: 0.0145\n",
      "Epoch 70: loss improved from 6.33795 to 6.30308, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.3031 - perplexity: 105.8391 - accuracy: 0.0145 - lr: 0.0010\n",
      "Epoch 71/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2695 - perplexity: 111.9311 - accuracy: 0.0157\n",
      "Epoch 71: loss improved from 6.30308 to 6.26954, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 6.2695 - perplexity: 111.9311 - accuracy: 0.0157 - lr: 0.0010\n",
      "Epoch 72/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.2270 - perplexity: 103.3508 - accuracy: 0.0157\n",
      "Epoch 72: loss improved from 6.26954 to 6.22705, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.2270 - perplexity: 103.3508 - accuracy: 0.0157 - lr: 0.0010\n",
      "Epoch 73/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1921 - perplexity: 98.4600 - accuracy: 0.0154\n",
      "Epoch 73: loss improved from 6.22705 to 6.19213, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.1921 - perplexity: 98.4600 - accuracy: 0.0154 - lr: 0.0010\n",
      "Epoch 74/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1549 - perplexity: 101.8314 - accuracy: 0.0169\n",
      "Epoch 74: loss improved from 6.19213 to 6.15492, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.1549 - perplexity: 101.8314 - accuracy: 0.0169 - lr: 0.0010\n",
      "Epoch 75/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.1215 - perplexity: 160.8641 - accuracy: 0.0167\n",
      "Epoch 75: loss improved from 6.15492 to 6.12150, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 6.1215 - perplexity: 160.8641 - accuracy: 0.0167 - lr: 0.0010\n",
      "Epoch 76/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0722 - perplexity: 96.3053 - accuracy: 0.0170\n",
      "Epoch 76: loss improved from 6.12150 to 6.07223, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 6.0722 - perplexity: 96.3053 - accuracy: 0.0170 - lr: 0.0010\n",
      "Epoch 77/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0418 - perplexity: 98.9068 - accuracy: 0.0175\n",
      "Epoch 77: loss improved from 6.07223 to 6.04178, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 6.0418 - perplexity: 98.9068 - accuracy: 0.0175 - lr: 0.0010\n",
      "Epoch 78/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 6.0205 - perplexity: 100.4091 - accuracy: 0.0187\n",
      "Epoch 78: loss improved from 6.04178 to 6.02045, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 6.0205 - perplexity: 100.4091 - accuracy: 0.0187 - lr: 0.0010\n",
      "Epoch 79/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9680 - perplexity: 100.8744 - accuracy: 0.0179\n",
      "Epoch 79: loss improved from 6.02045 to 5.96797, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 5.9680 - perplexity: 100.8744 - accuracy: 0.0179 - lr: 0.0010\n",
      "Epoch 80/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.9153 - perplexity: 115.2020 - accuracy: 0.0202\n",
      "Epoch 80: loss improved from 5.96797 to 5.91529, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 5.9153 - perplexity: 115.2020 - accuracy: 0.0202 - lr: 0.0010\n",
      "Epoch 81/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.8513 - perplexity: 85.7570 - accuracy: 0.0212\n",
      "Epoch 81: loss improved from 5.91529 to 5.85130, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 5.8513 - perplexity: 85.7570 - accuracy: 0.0212 - lr: 0.0010\n",
      "Epoch 82/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.8088 - perplexity: 215.9117 - accuracy: 0.0216\n",
      "Epoch 82: loss improved from 5.85130 to 5.80882, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 5.8088 - perplexity: 215.9117 - accuracy: 0.0216 - lr: 0.0010\n",
      "Epoch 83/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.7726 - perplexity: 31527.7656 - accuracy: 0.0229\n",
      "Epoch 83: loss improved from 5.80882 to 5.77256, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 5.7726 - perplexity: 31527.7656 - accuracy: 0.0229 - lr: 0.0010\n",
      "Epoch 84/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.6995 - perplexity: 78.2627 - accuracy: 0.0225\n",
      "Epoch 84: loss improved from 5.77256 to 5.69946, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 5.6995 - perplexity: 78.2627 - accuracy: 0.0225 - lr: 0.0010\n",
      "Epoch 85/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.6405 - perplexity: 84.1689 - accuracy: 0.0258\n",
      "Epoch 85: loss improved from 5.69946 to 5.64053, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 5.6405 - perplexity: 84.1689 - accuracy: 0.0258 - lr: 0.0010\n",
      "Epoch 86/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.5695 - perplexity: 2010.4438 - accuracy: 0.0258\n",
      "Epoch 86: loss improved from 5.64053 to 5.56951, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 5.5695 - perplexity: 2010.4438 - accuracy: 0.0258 - lr: 0.0010\n",
      "Epoch 87/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.5087 - perplexity: 111.1046 - accuracy: 0.0275\n",
      "Epoch 87: loss improved from 5.56951 to 5.50873, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 5.5087 - perplexity: 111.1046 - accuracy: 0.0275 - lr: 0.0010\n",
      "Epoch 88/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.4346 - perplexity: 87.0274 - accuracy: 0.0288\n",
      "Epoch 88: loss improved from 5.50873 to 5.43457, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 5.4346 - perplexity: 87.0274 - accuracy: 0.0288 - lr: 0.0010\n",
      "Epoch 89/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.3702 - perplexity: 122.3789 - accuracy: 0.0329\n",
      "Epoch 89: loss improved from 5.43457 to 5.37017, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 5.3702 - perplexity: 122.3789 - accuracy: 0.0329 - lr: 0.0010\n",
      "Epoch 90/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.3015 - perplexity: 813.7614 - accuracy: 0.0345\n",
      "Epoch 90: loss improved from 5.37017 to 5.30150, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 5.3015 - perplexity: 813.7614 - accuracy: 0.0345 - lr: 0.0010\n",
      "Epoch 91/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.2620 - perplexity: 135.3846 - accuracy: 0.0322\n",
      "Epoch 91: loss improved from 5.30150 to 5.26199, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 195ms/step - loss: 5.2620 - perplexity: 135.3846 - accuracy: 0.0322 - lr: 0.0010\n",
      "Epoch 92/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.2171 - perplexity: 396.9235 - accuracy: 0.0373\n",
      "Epoch 92: loss improved from 5.26199 to 5.21709, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 191ms/step - loss: 5.2171 - perplexity: 396.9235 - accuracy: 0.0373 - lr: 0.0010\n",
      "Epoch 93/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.1721 - perplexity: 1271.0189 - accuracy: 0.0364\n",
      "Epoch 93: loss improved from 5.21709 to 5.17209, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 5.1721 - perplexity: 1271.0189 - accuracy: 0.0364 - lr: 0.0010\n",
      "Epoch 94/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.1205 - perplexity: 170.6843 - accuracy: 0.0407\n",
      "Epoch 94: loss improved from 5.17209 to 5.12050, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 105s 7s/step - loss: 5.1205 - perplexity: 170.6843 - accuracy: 0.0407 - lr: 0.0010\n",
      "Epoch 95/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0811 - perplexity: 75.9280 - accuracy: 0.0398\n",
      "Epoch 95: loss improved from 5.12050 to 5.08108, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 205ms/step - loss: 5.0811 - perplexity: 75.9280 - accuracy: 0.0398 - lr: 0.0010\n",
      "Epoch 96/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0316 - perplexity: 67.0696 - accuracy: 0.0439\n",
      "Epoch 96: loss improved from 5.08108 to 5.03159, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 5.0316 - perplexity: 67.0696 - accuracy: 0.0439 - lr: 0.0010\n",
      "Epoch 97/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 5.0001 - perplexity: 97.3432 - accuracy: 0.0432\n",
      "Epoch 97: loss improved from 5.03159 to 5.00012, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 5.0001 - perplexity: 97.3432 - accuracy: 0.0432 - lr: 0.0010\n",
      "Epoch 98/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9546 - perplexity: 66.8078 - accuracy: 0.0457\n",
      "Epoch 98: loss improved from 5.00012 to 4.95462, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.9546 - perplexity: 66.8078 - accuracy: 0.0457 - lr: 0.0010\n",
      "Epoch 99/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.9155 - perplexity: 2453.1182 - accuracy: 0.0451\n",
      "Epoch 99: loss improved from 4.95462 to 4.91555, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.9155 - perplexity: 2453.1182 - accuracy: 0.0451 - lr: 0.0010\n",
      "Epoch 100/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8733 - perplexity: 111.6464 - accuracy: 0.0485\n",
      "Epoch 100: loss improved from 4.91555 to 4.87326, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.8733 - perplexity: 111.6464 - accuracy: 0.0485 - lr: 0.0010\n",
      "Epoch 101/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8389 - perplexity: 966.1097 - accuracy: 0.0483 \n",
      "Epoch 101: loss improved from 4.87326 to 4.83892, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 4.8389 - perplexity: 966.1097 - accuracy: 0.0483 - lr: 0.0010\n",
      "Epoch 102/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.8099 - perplexity: 839.0177 - accuracy: 0.0524\n",
      "Epoch 102: loss improved from 4.83892 to 4.80992, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 191ms/step - loss: 4.8099 - perplexity: 839.0177 - accuracy: 0.0524 - lr: 0.0010\n",
      "Epoch 103/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7621 - perplexity: 130.9367 - accuracy: 0.0541\n",
      "Epoch 103: loss improved from 4.80992 to 4.76214, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 4.7621 - perplexity: 130.9367 - accuracy: 0.0541 - lr: 0.0010\n",
      "Epoch 104/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7267 - perplexity: 56.2516 - accuracy: 0.0528\n",
      "Epoch 104: loss improved from 4.76214 to 4.72671, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.7267 - perplexity: 56.2516 - accuracy: 0.0528 - lr: 0.0010\n",
      "Epoch 105/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.7042 - perplexity: 7893.4282 - accuracy: 0.0574\n",
      "Epoch 105: loss improved from 4.72671 to 4.70417, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.7042 - perplexity: 7893.4282 - accuracy: 0.0574 - lr: 0.0010\n",
      "Epoch 106/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6715 - perplexity: 213.7297 - accuracy: 0.0578\n",
      "Epoch 106: loss improved from 4.70417 to 4.67154, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.6715 - perplexity: 213.7297 - accuracy: 0.0578 - lr: 0.0010\n",
      "Epoch 107/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6470 - perplexity: 2852.3887 - accuracy: 0.0595\n",
      "Epoch 107: loss improved from 4.67154 to 4.64697, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 4.6470 - perplexity: 2852.3887 - accuracy: 0.0595 - lr: 0.0010\n",
      "Epoch 108/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.6209 - perplexity: 733.5039 - accuracy: 0.0568\n",
      "Epoch 108: loss improved from 4.64697 to 4.62087, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.6209 - perplexity: 733.5039 - accuracy: 0.0568 - lr: 0.0010\n",
      "Epoch 109/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5935 - perplexity: 55.6988 - accuracy: 0.0637\n",
      "Epoch 109: loss improved from 4.62087 to 4.59355, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 4.5935 - perplexity: 55.6988 - accuracy: 0.0637 - lr: 0.0010\n",
      "Epoch 110/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5767 - perplexity: 174.2642 - accuracy: 0.0606\n",
      "Epoch 110: loss improved from 4.59355 to 4.57666, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 4.5767 - perplexity: 174.2642 - accuracy: 0.0606 - lr: 0.0010\n",
      "Epoch 111/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5500 - perplexity: 232.9641 - accuracy: 0.0668\n",
      "Epoch 111: loss improved from 4.57666 to 4.54996, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.5500 - perplexity: 232.9641 - accuracy: 0.0668 - lr: 0.0010\n",
      "Epoch 112/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5230 - perplexity: 131.4786 - accuracy: 0.0653\n",
      "Epoch 112: loss improved from 4.54996 to 4.52305, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 196ms/step - loss: 4.5230 - perplexity: 131.4786 - accuracy: 0.0653 - lr: 0.0010\n",
      "Epoch 113/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.5011 - perplexity: 832.0216 - accuracy: 0.0647\n",
      "Epoch 113: loss improved from 4.52305 to 4.50105, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 196ms/step - loss: 4.5011 - perplexity: 832.0216 - accuracy: 0.0647 - lr: 0.0010\n",
      "Epoch 114/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4702 - perplexity: 136.2766 - accuracy: 0.0667\n",
      "Epoch 114: loss improved from 4.50105 to 4.47023, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 194ms/step - loss: 4.4702 - perplexity: 136.2766 - accuracy: 0.0667 - lr: 0.0010\n",
      "Epoch 115/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4520 - perplexity: 218.9777 - accuracy: 0.0681\n",
      "Epoch 115: loss improved from 4.47023 to 4.45197, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.4520 - perplexity: 218.9777 - accuracy: 0.0681 - lr: 0.0010\n",
      "Epoch 116/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.4290 - perplexity: 91.3804 - accuracy: 0.0724\n",
      "Epoch 116: loss improved from 4.45197 to 4.42895, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 4.4290 - perplexity: 91.3804 - accuracy: 0.0724 - lr: 0.0010\n",
      "Epoch 117/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3983 - perplexity: 60.7659 - accuracy: 0.0724\n",
      "Epoch 117: loss improved from 4.42895 to 4.39833, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.3983 - perplexity: 60.7659 - accuracy: 0.0724 - lr: 0.0010\n",
      "Epoch 118/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3709 - perplexity: 118.7946 - accuracy: 0.0740\n",
      "Epoch 118: loss improved from 4.39833 to 4.37086, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 4.3709 - perplexity: 118.7946 - accuracy: 0.0740 - lr: 0.0010\n",
      "Epoch 119/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3558 - perplexity: 612.9639 - accuracy: 0.0755\n",
      "Epoch 119: loss improved from 4.37086 to 4.35584, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.3558 - perplexity: 612.9639 - accuracy: 0.0755 - lr: 0.0010\n",
      "Epoch 120/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3372 - perplexity: 448.2822 - accuracy: 0.0766\n",
      "Epoch 120: loss improved from 4.35584 to 4.33716, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.3372 - perplexity: 448.2822 - accuracy: 0.0766 - lr: 0.0010\n",
      "Epoch 121/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.3017 - perplexity: 37.3806 - accuracy: 0.0794\n",
      "Epoch 121: loss improved from 4.33716 to 4.30167, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.3017 - perplexity: 37.3806 - accuracy: 0.0794 - lr: 0.0010\n",
      "Epoch 122/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2790 - perplexity: 32.9312 - accuracy: 0.0791\n",
      "Epoch 122: loss improved from 4.30167 to 4.27903, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.2790 - perplexity: 32.9312 - accuracy: 0.0791 - lr: 0.0010\n",
      "Epoch 123/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2740 - perplexity: 73.0523 - accuracy: 0.0797\n",
      "Epoch 123: loss improved from 4.27903 to 4.27400, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.2740 - perplexity: 73.0523 - accuracy: 0.0797 - lr: 0.0010\n",
      "Epoch 124/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2691 - perplexity: 16157.8965 - accuracy: 0.0781\n",
      "Epoch 124: loss improved from 4.27400 to 4.26914, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.2691 - perplexity: 16157.8965 - accuracy: 0.0781 - lr: 0.0010\n",
      "Epoch 125/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2603 - perplexity: 41.8182 - accuracy: 0.0821\n",
      "Epoch 125: loss improved from 4.26914 to 4.26034, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 4.2603 - perplexity: 41.8182 - accuracy: 0.0821 - lr: 0.0010\n",
      "Epoch 126/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2544 - perplexity: 2449.8206 - accuracy: 0.0833\n",
      "Epoch 126: loss improved from 4.26034 to 4.25439, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.2544 - perplexity: 2449.8206 - accuracy: 0.0833 - lr: 0.0010\n",
      "Epoch 127/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2412 - perplexity: 1956.0200 - accuracy: 0.0824\n",
      "Epoch 127: loss improved from 4.25439 to 4.24118, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.2412 - perplexity: 1956.0200 - accuracy: 0.0824 - lr: 0.0010\n",
      "Epoch 128/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2250 - perplexity: 878.5094 - accuracy: 0.0843\n",
      "Epoch 128: loss improved from 4.24118 to 4.22503, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 4.2250 - perplexity: 878.5094 - accuracy: 0.0843 - lr: 0.0010\n",
      "Epoch 129/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.2114 - perplexity: 177.8862 - accuracy: 0.0811\n",
      "Epoch 129: loss improved from 4.22503 to 4.21144, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 190ms/step - loss: 4.2114 - perplexity: 177.8862 - accuracy: 0.0811 - lr: 0.0010\n",
      "Epoch 130/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1656 - perplexity: 75.2455 - accuracy: 0.0852\n",
      "Epoch 130: loss improved from 4.21144 to 4.16565, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.1656 - perplexity: 75.2455 - accuracy: 0.0852 - lr: 0.0010\n",
      "Epoch 131/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1414 - perplexity: 206.3627 - accuracy: 0.0877\n",
      "Epoch 131: loss improved from 4.16565 to 4.14137, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.1414 - perplexity: 206.3627 - accuracy: 0.0877 - lr: 0.0010\n",
      "Epoch 132/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1279 - perplexity: 28202.3418 - accuracy: 0.0886\n",
      "Epoch 132: loss improved from 4.14137 to 4.12793, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.1279 - perplexity: 28202.3418 - accuracy: 0.0886 - lr: 0.0010\n",
      "Epoch 133/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1318 - perplexity: 60.1775 - accuracy: 0.0864\n",
      "Epoch 133: loss did not improve from 4.12793\n",
      "15/15 [==============================] - 3s 172ms/step - loss: 4.1318 - perplexity: 60.1775 - accuracy: 0.0864 - lr: 0.0010\n",
      "Epoch 134/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1152 - perplexity: 34008.7305 - accuracy: 0.0880\n",
      "Epoch 134: loss improved from 4.12793 to 4.11516, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 4.1152 - perplexity: 34008.7305 - accuracy: 0.0880 - lr: 0.0010\n",
      "Epoch 135/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.1165 - perplexity: 100.4209 - accuracy: 0.0871\n",
      "Epoch 135: loss did not improve from 4.11516\n",
      "15/15 [==============================] - 3s 173ms/step - loss: 4.1165 - perplexity: 100.4209 - accuracy: 0.0871 - lr: 0.0010\n",
      "Epoch 136/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0854 - perplexity: 443.5609 - accuracy: 0.0909\n",
      "Epoch 136: loss improved from 4.11516 to 4.08545, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.0854 - perplexity: 443.5609 - accuracy: 0.0909 - lr: 0.0010\n",
      "Epoch 137/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0687 - perplexity: 210.6299 - accuracy: 0.0917\n",
      "Epoch 137: loss improved from 4.08545 to 4.06868, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 4.0687 - perplexity: 210.6299 - accuracy: 0.0917 - lr: 0.0010\n",
      "Epoch 138/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0532 - perplexity: 6838.4204 - accuracy: 0.0904\n",
      "Epoch 138: loss improved from 4.06868 to 4.05316, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 4.0532 - perplexity: 6838.4204 - accuracy: 0.0904 - lr: 0.0010\n",
      "Epoch 139/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0380 - perplexity: 6967.7759 - accuracy: 0.0942\n",
      "Epoch 139: loss improved from 4.05316 to 4.03801, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 4.0380 - perplexity: 6967.7759 - accuracy: 0.0942 - lr: 0.0010\n",
      "Epoch 140/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 4.0124 - perplexity: 762.1766 - accuracy: 0.0951\n",
      "Epoch 140: loss improved from 4.03801 to 4.01245, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 4.0124 - perplexity: 762.1766 - accuracy: 0.0951 - lr: 0.0010\n",
      "Epoch 141/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9893 - perplexity: 39.1680 - accuracy: 0.0986\n",
      "Epoch 141: loss improved from 4.01245 to 3.98926, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 3.9893 - perplexity: 39.1680 - accuracy: 0.0986 - lr: 0.0010\n",
      "Epoch 142/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9755 - perplexity: 100.3394 - accuracy: 0.0950\n",
      "Epoch 142: loss improved from 3.98926 to 3.97548, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 189ms/step - loss: 3.9755 - perplexity: 100.3394 - accuracy: 0.0950 - lr: 0.0010\n",
      "Epoch 143/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9710 - perplexity: 2404.2239 - accuracy: 0.0949\n",
      "Epoch 143: loss improved from 3.97548 to 3.97100, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 3.9710 - perplexity: 2404.2239 - accuracy: 0.0949 - lr: 0.0010\n",
      "Epoch 144/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9486 - perplexity: 93.2179 - accuracy: 0.0942\n",
      "Epoch 144: loss improved from 3.97100 to 3.94865, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 3.9486 - perplexity: 93.2179 - accuracy: 0.0942 - lr: 0.0010\n",
      "Epoch 145/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9449 - perplexity: 2379.1594 - accuracy: 0.0972\n",
      "Epoch 145: loss improved from 3.94865 to 3.94486, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 3.9449 - perplexity: 2379.1594 - accuracy: 0.0972 - lr: 0.0010\n",
      "Epoch 146/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.9223 - perplexity: 1177929.2500 - accuracy: 0.1023\n",
      "Epoch 146: loss improved from 3.94486 to 3.92227, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 3.9223 - perplexity: 1177929.2500 - accuracy: 0.1023 - lr: 0.0010\n",
      "Epoch 147/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.8974 - perplexity: 48.9699 - accuracy: 0.1073\n",
      "Epoch 147: loss improved from 3.92227 to 3.89736, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 3.8974 - perplexity: 48.9699 - accuracy: 0.1073 - lr: 0.0010\n",
      "Epoch 148/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.8822 - perplexity: 40.0745 - accuracy: 0.1051\n",
      "Epoch 148: loss improved from 3.89736 to 3.88224, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 3.8822 - perplexity: 40.0745 - accuracy: 0.1051 - lr: 0.0010\n",
      "Epoch 149/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.8714 - perplexity: 63408.0000 - accuracy: 0.1081\n",
      "Epoch 149: loss improved from 3.88224 to 3.87139, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 3.8714 - perplexity: 63408.0000 - accuracy: 0.1081 - lr: 0.0010\n",
      "Epoch 150/150\n",
      "15/15 [==============================] - ETA: 0s - loss: 3.8517 - perplexity: 58.8431 - accuracy: 0.1073\n",
      "Epoch 150: loss improved from 3.87139 to 3.85168, saving model to ../model/nextword1.h5\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 3.8517 - perplexity: 58.8431 - accuracy: 0.1073 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=150, batch_size=1000, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'perplexity', 'accuracy', 'lr'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting training and validation losses\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['perplexity'][0:80]\n",
    "#val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, 81)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'o',c = \"#d95f02\", label='Perplexity')\n",
    "#plt.plot(epochs, val_loss_values, '-',c = '#d95f02', label='Validation loss')\n",
    "plt.title('Training Perplexity')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "plt.savefig(\"../plots/perplexity_lstm_output.png\") #save as png\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"../model/nextword.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../model/nextword_avi.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE REFERENCE: \n",
    "https://www.kaggle.com/code/ysthehurricane/next-word-prediction-bi-lstm-tutorial-easy-way <br>\n",
    "https://www.analyticsvidhya.com/blog/2021/08/predict-the-next-word-of-your-text-using-long-short-term-memory-lstm/ <br>\n",
    "https://github.com/Bharath-K3/Next-Word-Prediction-with-NLP-and-Deep-Learning <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 02:55:11.370057: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = pickle.load(open('../model/tokenizer1.pkl', 'rb'))\n",
    "# load json and create model\n",
    "json_file = open('../model/nextword.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"../model/nextword.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('anly-580')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5abdb303892873824d5077e7ac4d83088b903b32bdc9644b35b87d07f0c1fc70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
