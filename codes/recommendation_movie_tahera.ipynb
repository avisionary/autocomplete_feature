{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cc5e08",
   "metadata": {},
   "source": [
    "# Movie recommendation system\n",
    "Explore different models for semantic text matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a0a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ccc88",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "Data is already prepared in folder ``data`` of this repository. Source: https://www.kaggle.com/rounakbanik/the-movies-dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f40f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tahera/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15602</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>en</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>False</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31357</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>en</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11862</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>en</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>76578911.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
       "3  False                                                NaN  16000000   \n",
       "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                               homepage     id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
       "1                                   NaN   8844  tt0113497                en   \n",
       "2                                   NaN  15602  tt0113228                en   \n",
       "3                                   NaN  31357  tt0114885                en   \n",
       "4                                   NaN  11862  tt0113041                en   \n",
       "\n",
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            sentence  ... release_date  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
       "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
       "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
       "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
       "\n",
       "       revenue runtime                                   spoken_languages  \\\n",
       "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
       "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "\n",
       "     status                                            tagline  \\\n",
       "0  Released                                                NaN   \n",
       "1  Released          Roll the dice and unleash the excitement!   \n",
       "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Released  Friends are the people who let you be yourself...   \n",
       "4  Released  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                         title  video vote_average vote_count  \n",
       "0                    Toy Story  False          7.7     5415.0  \n",
       "1                      Jumanji  False          6.9     2413.0  \n",
       "2             Grumpier Old Men  False          6.5       92.0  \n",
       "3            Waiting to Exhale  False          6.1       34.0  \n",
       "4  Father of the Bride Part II  False          5.7      173.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATAPATH = '../data/movies_metadata.csv' \n",
    "\n",
    "df = pd.read_csv(DATAPATH)\n",
    "df = df[~df.overview.isna()]\n",
    "df.rename(columns={'overview':'sentence'}, inplace=True)\n",
    "print(len(df))\n",
    "df = df.iloc[:20000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0a0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe41ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning sentences...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 4\n",
    "MAX_WORDS = 200\n",
    "\n",
    "PATTERN_S = re.compile(\"\\'s\")  # matches `'s` from text  \n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\") #matches `\\r` and `\\n`\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\") # matches all non 0-9 A-z whitespace \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Series of cleaning. String to lower case, remove non words characters and numbers.\n",
    "        text (str): input text\n",
    "    return (str): modified initial text\n",
    "    \"\"\"\n",
    "    text = text.lower()  # lowercase text\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    return text\n",
    "\n",
    "def tokenizer(sentence, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Lemmatize, tokenize, crop and remove stop words.\n",
    "    \"\"\"\n",
    "    if lemmatize:\n",
    "        stemmer = WordNetLemmatizer()\n",
    "        tokens = [stemmer.lemmatize(w) for w in word_tokenize(sentence)]\n",
    "    else:\n",
    "        tokens = [w for w in word_tokenize(sentence)]\n",
    "    token = [w for w in tokens if (len(w) > min_words and len(w) < max_words\n",
    "                                                        and w not in stopwords)]\n",
    "    return tokens    \n",
    "\n",
    "\n",
    "def clean_sentences(df):\n",
    "    \"\"\"\n",
    "    Remove irrelavant characters (in new column clean_sentence).\n",
    "    Lemmatize, tokenize words into list of words (in new column tok_lem_sentence).\n",
    "    \"\"\"\n",
    "    print('Cleaning sentences...')\n",
    "    df['clean_sentence'] = df['sentence'].apply(clean_text)\n",
    "    df['tok_lem_sentence'] = df['clean_sentence'].apply(\n",
    "        lambda x: tokenizer(x, min_words=MIN_WORDS, max_words=MAX_WORDS, stopwords=STOPWORDS, lemmatize=True))\n",
    "    return df\n",
    "    \n",
    "df = clean_sentences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed392d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>clean_sentence</th>\n",
       "      <th>tok_lem_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>led by woody  andy  toys live happily in his r...</td>\n",
       "      <td>[led, by, woody, andy, toy, live, happily, in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>when siblings judy and peter discover an encha...</td>\n",
       "      <td>[when, sibling, judy, and, peter, discover, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>a family wedding reignites the ancient feud be...</td>\n",
       "      <td>[a, family, wedding, reignites, the, ancient, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>cheated on  mistreated and stepped on  the wom...</td>\n",
       "      <td>[cheated, on, mistreated, and, stepped, on, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>just when george banks has recovered from his ...</td>\n",
       "      <td>[just, when, george, bank, ha, recovered, from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20131</th>\n",
       "      <td>After a lifetime of hiding, Chely Wright becom...</td>\n",
       "      <td>after a lifetime of hiding  chely wright becom...</td>\n",
       "      <td>[after, a, lifetime, of, hiding, chely, wright...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20132</th>\n",
       "      <td>In 1989, five black and Latino teenagers from ...</td>\n",
       "      <td>in 1989  five black and latino teenagers from ...</td>\n",
       "      <td>[in, 1989, five, black, and, latino, teenager,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20133</th>\n",
       "      <td>Arkin escapes with his life from the vicious g...</td>\n",
       "      <td>arkin escapes with his life from the vicious g...</td>\n",
       "      <td>[arkin, escape, with, his, life, from, the, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20134</th>\n",
       "      <td>Remake of a hit film from 1990, \"The Cherry Or...</td>\n",
       "      <td>remake of a hit film from 1990   the cherry or...</td>\n",
       "      <td>[remake, of, a, hit, film, from, 1990, the, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20135</th>\n",
       "      <td>Chronicles the adventures of Franklin Delano R...</td>\n",
       "      <td>chronicles the adventures of franklin delano r...</td>\n",
       "      <td>[chronicle, the, adventure, of, franklin, dela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      Led by Woody, Andy's toys live happily in his ...   \n",
       "1      When siblings Judy and Peter discover an encha...   \n",
       "2      A family wedding reignites the ancient feud be...   \n",
       "3      Cheated on, mistreated and stepped on, the wom...   \n",
       "4      Just when George Banks has recovered from his ...   \n",
       "...                                                  ...   \n",
       "20131  After a lifetime of hiding, Chely Wright becom...   \n",
       "20132  In 1989, five black and Latino teenagers from ...   \n",
       "20133  Arkin escapes with his life from the vicious g...   \n",
       "20134  Remake of a hit film from 1990, \"The Cherry Or...   \n",
       "20135  Chronicles the adventures of Franklin Delano R...   \n",
       "\n",
       "                                          clean_sentence  \\\n",
       "0      led by woody  andy  toys live happily in his r...   \n",
       "1      when siblings judy and peter discover an encha...   \n",
       "2      a family wedding reignites the ancient feud be...   \n",
       "3      cheated on  mistreated and stepped on  the wom...   \n",
       "4      just when george banks has recovered from his ...   \n",
       "...                                                  ...   \n",
       "20131  after a lifetime of hiding  chely wright becom...   \n",
       "20132  in 1989  five black and latino teenagers from ...   \n",
       "20133  arkin escapes with his life from the vicious g...   \n",
       "20134  remake of a hit film from 1990   the cherry or...   \n",
       "20135  chronicles the adventures of franklin delano r...   \n",
       "\n",
       "                                        tok_lem_sentence  \n",
       "0      [led, by, woody, andy, toy, live, happily, in,...  \n",
       "1      [when, sibling, judy, and, peter, discover, an...  \n",
       "2      [a, family, wedding, reignites, the, ancient, ...  \n",
       "3      [cheated, on, mistreated, and, stepped, on, th...  \n",
       "4      [just, when, george, bank, ha, recovered, from...  \n",
       "...                                                  ...  \n",
       "20131  [after, a, lifetime, of, hiding, chely, wright...  \n",
       "20132  [in, 1989, five, black, and, latino, teenager,...  \n",
       "20133  [arkin, escape, with, his, life, from, the, vi...  \n",
       "20134  [remake, of, a, hit, film, from, 1990, the, ch...  \n",
       "20135  [chronicle, the, adventure, of, franklin, dela...  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df[['sentence', 'clean_sentence', 'tok_lem_sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd8b77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['clean_sentence']].to_csv('clean_movies.txt', index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a87bbf",
   "metadata": {},
   "source": [
    "## Query sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7f75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sentence = 'a crime story with a beautiful woman' \n",
    "\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55423f",
   "metadata": {},
   "source": [
    "## Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1112d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_indices(m, topk, mask=None):\n",
    "    \"\"\"\n",
    "    Use sum of the cosine distance over all tokens.\n",
    "    m (np.array): cos matrix of shape (nb_in_tokens, nb_dict_tokens)\n",
    "    topk (int): number of indices to return (from high to lowest in order)\n",
    "    \"\"\"\n",
    "    # return the sum on all tokens of cosinus for each sentence\n",
    "    if len(m.shape) > 1:\n",
    "        cos_sim = np.mean(m, axis=0) \n",
    "    else: \n",
    "        cos_sim = m\n",
    "    index = np.argsort(cos_sim)[::-1] # from highest idx to smallest score \n",
    "    if mask is not None:\n",
    "        assert mask.shape == m.shape\n",
    "        mask = mask[index]\n",
    "    else:\n",
    "        mask = np.ones(len(cos_sim))\n",
    "    mask = np.logical_or(cos_sim[index] != 0, mask) #eliminate 0 cosine distance\n",
    "    best_index = index[mask][:topk]  \n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdde677b",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaea497",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a046d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000, 49932)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Adapt stop words\n",
    "token_stop = tokenizer(' '.join(STOPWORDS), lemmatize=False)\n",
    "\n",
    "# Fit TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer) \n",
    "tfidf_mat = vectorizer.fit_transform(df['sentence'].values) # -> (num_sentences, num_vocabulary)\n",
    "tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3193b01e",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ad1f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a crime story with a beautiful woman'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ae4e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 20000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>Innocent Blood</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 27, 'name': 'Horror'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]</td>\n",
       "      <td>A beautiful vampire turns a crime lord into a creature of the night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10493</th>\n",
       "      <td>Requiem pour un vampire</td>\n",
       "      <td>[{'id': 27, 'name': 'Horror'}]</td>\n",
       "      <td>A vampire lures beautiful young women to his castle in Europe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18224</th>\n",
       "      <td>Miss Bala</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}]</td>\n",
       "      <td>The story of a young woman clinging on to her dream to become a beauty contest queen in a Mexico dominated by organized crime.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                original_title  \\\n",
       "9003            Innocent Blood   \n",
       "10493  Requiem pour un vampire   \n",
       "18224                Miss Bala   \n",
       "\n",
       "                                                                                                                          genres  \\\n",
       "9003   [{'id': 35, 'name': 'Comedy'}, {'id': 27, 'name': 'Horror'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]   \n",
       "10493                                                                                             [{'id': 27, 'name': 'Horror'}]   \n",
       "18224                                                                [{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}]   \n",
       "\n",
       "                                                                                                                             sentence  \n",
       "9003                                                             A beautiful vampire turns a crime lord into a creature of the night.  \n",
       "10493                                                                  A vampire lures beautiful young women to his castle in Europe.  \n",
       "18224  The story of a young woman clinging on to her dream to become a beauty contest queen in a Mexico dominated by organized crime.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_recommendations_tfidf(sentence, tfidf_mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the database sentences in order of highest cosine similarity relatively to each \n",
    "    token of the target sentence. \n",
    "    \"\"\"\n",
    "    # Embed the query sentence\n",
    "    tokens = [str(tok) for tok in tokenizer(sentence)]\n",
    "    vec = vectorizer.transform(tokens)\n",
    "    # Create list with similarity between query and dataset\n",
    "    mat = cosine_similarity(vec, tfidf_mat)\n",
    "    # Best cosine distance for each token independantly\n",
    "    print(mat.shape)\n",
    "    best_index = extract_best_indices(mat, topk=3)\n",
    "    return best_index\n",
    "\n",
    "\n",
    "best_index = get_recommendations_tfidf(query_sentence, tfidf_mat)\n",
    "\n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85952a",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cebd0",
   "metadata": {},
   "source": [
    "## Load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca87b62f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Tahera/Desktop/ANLY 580/Project/movie_recommendation/recommendation_movie.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m nlp \u001b[39m=\u001b[39m en_core_web_sm\u001b[39m.\u001b[39mload()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Apply the model to the sentences\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mspacy_sentence\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39msentence\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: nlp(x)) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Retrieve the embedded vectors as a matrix \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m embed_mat \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mspacy_sentence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/pandas/core/series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4248\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4252\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4253\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FrameOrSeriesUnion:\n\u001b[1;32m   4254\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4256\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4355\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4357\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/pandas/core/apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1040\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1043\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/pandas/core/apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1093\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1099\u001b[0m             values,\n\u001b[1;32m   1100\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1101\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1102\u001b[0m         )\n\u001b[1;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1105\u001b[0m     \u001b[39m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m     \u001b[39m# so extension arrays can be used\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/Tahera/Desktop/ANLY 580/Project/movie_recommendation/recommendation_movie.ipynb Cell 20\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m nlp \u001b[39m=\u001b[39m en_core_web_sm\u001b[39m.\u001b[39mload()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Apply the model to the sentences\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mspacy_sentence\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: nlp(x)) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Retrieve the embedded vectors as a matrix \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m embed_mat \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mspacy_sentence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/spacy/language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    126\u001b[0m batch_id \u001b[39m=\u001b[39m Tok2VecListener\u001b[39m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m listener \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/with_array.py:32\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     29\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     30\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[0;32m---> 32\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n\u001b[1;32m     33\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[1;32m     34\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/with_array.py:87\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[1;32m     84\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[1;32m     85\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[1;32m     86\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 87\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[1;32m     89\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[1;32m     90\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/layernorm.py:27\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     25\u001b[0m N, mu, var \u001b[39m=\u001b[39m _get_moments(model\u001b[39m.\u001b[39mops, X)\n\u001b[1;32m     26\u001b[0m Xhat \u001b[39m=\u001b[39m (X \u001b[39m-\u001b[39m mu) \u001b[39m*\u001b[39m var \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39m2.0\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m Y, backprop_rescale \u001b[39m=\u001b[39m _begin_update_scale_shift(model, Xhat)\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[1;32m     30\u001b[0m     dY \u001b[39m=\u001b[39m backprop_rescale(dY)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/thinc/layers/layernorm.py:63\u001b[0m, in \u001b[0;36m_begin_update_scale_shift\u001b[0;34m(model, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m b \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m Y \u001b[39m=\u001b[39m X \u001b[39m*\u001b[39m G\n\u001b[0;32m---> 63\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m b\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfinish_update_scale_shift\u001b[39m(dY: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[1;32m     66\u001b[0m     model\u001b[39m.\u001b[39minc_grad(\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m, dY\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "#Load pre-trained model\n",
    "#nlp = spacy.load(\"en_core_web_lg\") \n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "# Apply the model to the sentences\n",
    "df['spacy_sentence'] = df['sentence'].apply(lambda x: nlp(x)) \n",
    "# Retrieve the embedded vectors as a matrix \n",
    "embed_mat = df['spacy_sentence'].values\n",
    "embed_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hl/8cs69m1n759_xb3bt3fkk4982d1l4s/T/ipykernel_47525/1677401613.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  mat = np.array([query_embed.similarity(line) for line in embed_mat])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9003</th>\n",
       "      <td>Innocent Blood</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 27, 'name': 'Horror'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]</td>\n",
       "      <td>A beautiful vampire turns a crime lord into a creature of the night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10493</th>\n",
       "      <td>Requiem pour un vampire</td>\n",
       "      <td>[{'id': 27, 'name': 'Horror'}]</td>\n",
       "      <td>A vampire lures beautiful young women to his castle in Europe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18224</th>\n",
       "      <td>Miss Bala</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}]</td>\n",
       "      <td>The story of a young woman clinging on to her dream to become a beauty contest queen in a Mexico dominated by organized crime.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                original_title  \\\n",
       "9003            Innocent Blood   \n",
       "10493  Requiem pour un vampire   \n",
       "18224                Miss Bala   \n",
       "\n",
       "                                                                                                                          genres  \\\n",
       "9003   [{'id': 35, 'name': 'Comedy'}, {'id': 27, 'name': 'Horror'}, {'id': 53, 'name': 'Thriller'}, {'id': 80, 'name': 'Crime'}]   \n",
       "10493                                                                                             [{'id': 27, 'name': 'Horror'}]   \n",
       "18224                                                                [{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}]   \n",
       "\n",
       "                                                                                                                             sentence  \n",
       "9003                                                             A beautiful vampire turns a crime lord into a creature of the night.  \n",
       "10493                                                                  A vampire lures beautiful young women to his castle in Europe.  \n",
       "18224  The story of a young woman clinging on to her dream to become a beauty contest queen in a Mexico dominated by organized crime.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_spacy(model, query_sentence, embed_mat, topk=5):\n",
    "    \"\"\"\n",
    "    Predict the topk sentences after applying spacy model.\n",
    "    \"\"\"\n",
    "    query_embed = model(query_sentence)\n",
    "    mat = np.array([query_embed.similarity(line) for line in embed_mat])\n",
    "    # keep if vector has a norm\n",
    "    mat_mask = np.array(\n",
    "        [True if line.vector_norm else False for line in embed_mat])\n",
    "    best_index = extract_best_indices(mat, topk=topk, mask=mat_mask)\n",
    "    return best_index\n",
    "\n",
    "# Predict\n",
    "predict_spacy(nlp, query_sentence, embed_mat)\n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443ec88",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e687e9d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "768d6747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25184905, 33081030)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Create model\n",
    "word2vec_model = Word2Vec(min_count=0, workers = 8, vector_size=300) \n",
    "# Prepare vocab\n",
    "word2vec_model.build_vocab(df.tok_lem_sentence.values)\n",
    "# Train\n",
    "word2vec_model.train(df.tok_lem_sentence.values, total_examples=word2vec_model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535ee9f",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aab7f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9232</th>\n",
       "      <td>苏州河</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}, {'id': 10769, 'name': 'Foreign'}, {'id': 10749, 'name': 'Romance'}]</td>\n",
       "      <td>A tragic love story set in contemporary Shanghai. The film stars Zhou Xun in a dual role as two different women and Jia Hongsheng as a man obsessed with finding a woman from his past.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Heavy Metal</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 878, 'name': 'Science Fiction'}]</td>\n",
       "      <td>A glowing orb terrorizes a young girl with a collection of stories of dark fantasy, eroticism and horror.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>La sirène du Mississipi</td>\n",
       "      <td>[{'id': 80, 'name': 'Crime'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]</td>\n",
       "      <td>Adapted from a story by William Irish, it's a noirish tale of a man who orders a mail-order bride but receives instead a con woman.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                original_title  \\\n",
       "9232                       苏州河   \n",
       "602                Heavy Metal   \n",
       "10786  La sirène du Mississipi   \n",
       "\n",
       "                                                                                                  genres  \\\n",
       "9232   [{'id': 18, 'name': 'Drama'}, {'id': 10769, 'name': 'Foreign'}, {'id': 10749, 'name': 'Romance'}]   \n",
       "602                            [{'id': 16, 'name': 'Animation'}, {'id': 878, 'name': 'Science Fiction'}]   \n",
       "10786       [{'id': 80, 'name': 'Crime'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]   \n",
       "\n",
       "                                                                                                                                                                                      sentence  \n",
       "9232   A tragic love story set in contemporary Shanghai. The film stars Zhou Xun in a dual role as two different women and Jia Hongsheng as a man obsessed with finding a woman from his past.  \n",
       "602                                                                                  A glowing orb terrorizes a young girl with a collection of stories of dark fantasy, eroticism and horror.  \n",
       "10786                                                      Adapted from a story by William Irish, it's a noirish tale of a man who orders a mail-order bride but receives instead a con woman.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def is_word_in_model(word, model):\n",
    "    \"\"\"\n",
    "    Check on individual words ``word`` that it exists in ``model``.\n",
    "    \"\"\"\n",
    "    assert type(model).__name__ == 'KeyedVectors'\n",
    "    is_in_vocab = word in model.key_to_index.keys()\n",
    "    return is_in_vocab\n",
    "\n",
    "def predict_w2v(query_sentence, dataset, model, topk=3):\n",
    "    query_sentence = query_sentence.split()\n",
    "    in_vocab_list, best_index = [], [0]*topk\n",
    "    for w in query_sentence:\n",
    "        # remove unseen words from query sentence\n",
    "        if is_word_in_model(w, model.wv):\n",
    "            in_vocab_list.append(w)\n",
    "    # Retrieve the similarity between two words as a distance\n",
    "    if len(in_vocab_list) > 0:\n",
    "        sim_mat = np.zeros(len(dataset))  # TO DO\n",
    "        for i, data_sentence in enumerate(dataset):\n",
    "            if data_sentence:\n",
    "                sim_sentence = model.wv.n_similarity(\n",
    "                        in_vocab_list, data_sentence)\n",
    "            else:\n",
    "                sim_sentence = 0\n",
    "            sim_mat[i] = np.array(sim_sentence)\n",
    "        # Take the five highest norm\n",
    "        best_index = np.argsort(sim_mat)[::-1][:topk]\n",
    "    return best_index\n",
    "\n",
    "# Predict\n",
    "best_index = predict_w2v(query_sentence, df['tok_lem_sentence'].values, word2vec_model)    \n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48022680",
   "metadata": {},
   "source": [
    "# Transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9ce9b",
   "metadata": {},
   "source": [
    "## Load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c98cfed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     |████████████████████████████████| 85 kB 14 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (4.24.0)\n",
      "Requirement already satisfied: tqdm in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (1.12.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.0-cp39-cp39-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "     |████████████████████████████████| 1.4 MB 190 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (1.22.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (1.7.3)\n",
      "Requirement already satisfied: nltk in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 54 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from sentence_transformers) (0.10.1)\n",
      "Requirement already satisfied: requests in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.0.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.3.15)\n",
      "Requirement already satisfied: click in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from nltk->sentence_transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from torchvision->sentence_transformers) (8.4.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-1.13.0-cp39-none-macosx_10_9_x86_64.whl (137.9 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Tahera/opt/anaconda3/envs/anly503/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2021.10.8)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=4e05774a39970066de5455c9f98acac24213ecda63c949d54be0b6c912d92e66\n",
      "  Stored in directory: /Users/Tahera/Library/Caches/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: torch, torchvision, sentencepiece, sentence-transformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.97 torch-1.13.0 torchvision-0.14.0\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "072eba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 690/690 [00:00<00:00, 106kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 44.9kB/s]\n",
      "Downloading: 100%|██████████| 3.69k/3.69k [00:00<00:00, 662kB/s]\n",
      "Downloading: 100%|██████████| 629/629 [00:00<00:00, 65.6kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 37.2kB/s]\n",
      "Downloading:  70%|██████▉   | 63.5M/90.9M [02:44<00:31, 866kB/s]   "
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/urllib3/response.py:438\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    441\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/urllib3/response.py:519\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    518\u001b[0m cache_content \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    521\u001b[0m     amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data\n\u001b[1;32m    522\u001b[0m ):  \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/requests/models.py:760\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 760\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    761\u001b[0m         \u001b[39myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/urllib3/response.py:576\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 576\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    578\u001b[0m     \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/urllib3/response.py:541\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menforce_content_length \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\n\u001b[1;32m    533\u001b[0m                 \u001b[39m0\u001b[39m,\n\u001b[1;32m    534\u001b[0m                 \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[39m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    540\u001b[0m                 \u001b[39m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m                 \u001b[39mraise\u001b[39;00m IncompleteRead(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp_bytes_read, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_remaining)\n\u001b[1;32m    543\u001b[0m \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/urllib3/response.py:443\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    441\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mRead timed out.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m BaseSSLError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/Tahera/Desktop/ANLY 580/Project/movie_recommendation/recommendation_movie.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentence_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m SentenceTransformer(\u001b[39m'\u001b[39;49m\u001b[39mparaphrase-MiniLM-L6-v2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m corpus_embeddings \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(df\u001b[39m.\u001b[39msentence\u001b[39m.\u001b[39mvalues, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m query_embedding \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode(query_sentence, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:87\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     83\u001b[0m     model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m     86\u001b[0m         \u001b[39m# Download from hub with caching\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39;49mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msentence-transformers\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39;49m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mflax_model.msgpack\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrust_model.ot\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtf_model.h5\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/sentence_transformers/util.py:491\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(huggingface_hub\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m0.8.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    487\u001b[0m     \u001b[39m# huggingface_hub v0.8.1 introduces a new cache layout. We sill use a manual layout\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     \u001b[39m# And need to pass legacy_cache_layout=True to avoid that a warning will be printed\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     cached_download_args[\u001b[39m'\u001b[39m\u001b[39mlegacy_cache_layout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m path \u001b[39m=\u001b[39m cached_download(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_download_args)\n\u001b[1;32m    493\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    494\u001b[0m     os\u001b[39m.\u001b[39mremove(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/huggingface_hub/file_download.py:737\u001b[0m, in \u001b[0;36mcached_download\u001b[0;34m(url, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m    735\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 737\u001b[0m     http_get(\n\u001b[1;32m    738\u001b[0m         url_to_download,\n\u001b[1;32m    739\u001b[0m         temp_file,\n\u001b[1;32m    740\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    741\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    742\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    743\u001b[0m     )\n\u001b[1;32m    745\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, cache_path)\n\u001b[1;32m    746\u001b[0m os\u001b[39m.\u001b[39mreplace(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/huggingface_hub/file_download.py:490\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    481\u001b[0m total \u001b[39m=\u001b[39m resume_size \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(content_length) \u001b[39mif\u001b[39;00m content_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    482\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    483\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    484\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    489\u001b[0m )\n\u001b[0;32m--> 490\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/requests/models.py:767\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[1;32m    766\u001b[0m     \u001b[39mexcept\u001b[39;00m ReadTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    768\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     \u001b[39m# Standard file-like object.\u001b[39;00m\n\u001b[1;32m    770\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "corpus_embeddings = model.encode(df.sentence.values, convert_to_tensor=True)\n",
    "query_embedding = model.encode(query_sentence, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c127179",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1f2ea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: a crime story with a beautiful woman\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "original_title                                                                                                                         Miss Bala\n",
       "genres                                                                               [{'id': 18, 'name': 'Drama'}, {'id': 28, 'name': 'Action'}]\n",
       "sentence          The story of a young woman clinging on to her dream to become a beauty contest queen in a Mexico dominated by organized crime.\n",
       "Name: 18224, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "original_title                                                                                                                                                                                                                                                                                                                                 In the Cut\n",
       "genres                                                                                                                                                                                                                                                                                  [{'id': 9648, 'name': 'Mystery'}, {'id': 53, 'name': 'Thriller'}]\n",
       "sentence          Following the gruesome murder of a young woman in her neighborhood, a self-determined woman living in New York City--as if to test the limits of her own safety--propels herself into an impossibly risky sexual liaison. Soon she grows increasingly wary about the motives of every man with whom she has contact--and about her own.\n",
       "Name: 6736, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "original_title                                                                                    The World of Suzie Wong\n",
       "genres                                                    [{'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]\n",
       "sentence          Story of the love between a struggling American artist and a beautiful Chinese prostitute in Hong Kong.\n",
       "Name: 8018, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# We use cosine-similarity and torch.topk to find the highest 3 scores\n",
    "cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "top_results = torch.topk(cos_scores, k=3)\n",
    "\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"Query:\", query_sentence)\n",
    "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    score = score.cpu().data.numpy() \n",
    "    idx = idx.cpu().data.numpy()\n",
    "    display(df[['original_title', 'genres', 'sentence']].iloc[idx])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097d9ab",
   "metadata": {},
   "source": [
    "# BERT with hugging face\n",
    "We write a class to load the dataset, embed it in the object and use is to compute distance with embed query sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd5a92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "BERT_BATCH_SIZE = 4\n",
    "MODEL_NAME = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "class BertModel:\n",
    "    def __init__(self, model_name, device=-1, small_memory=True, batch_size=BERT_BATCH_SIZE):\n",
    "        self.model_name = model_name\n",
    "        self._set_device(device)\n",
    "        self.small_device = 'cpu' if small_memory else self.device\n",
    "        self.batch_size = batch_size\n",
    "        self.load_pretrained_model()\n",
    "\n",
    "    def _set_device(self, device):\n",
    "        if device == -1 or device == 'cpu':\n",
    "            self.device = 'cpu'\n",
    "        elif device == 'cuda' or device == 'gpu':\n",
    "            self.device = 'cuda'\n",
    "        elif isinstance(device, int) or isinstance(device, float):\n",
    "            self.device = 'cuda'\n",
    "        else:  # default\n",
    "            self.device = torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def load_pretrained_model(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        device = -1 if self.device == 'cpu' else 0\n",
    "        self.pipeline = pipeline('feature-extraction',\n",
    "                                 model=self.model, tokenizer=self.tokenizer, device=device)\n",
    "\n",
    "    def embed(self, data):\n",
    "        \"\"\" Create the embedded matrice from original sentences \"\"\"\n",
    "        nb_batchs = 1 if (len(data) < self.batch_size) else len(\n",
    "            data) // self.batch_size\n",
    "        batchs = np.array_split(data, nb_batchs)\n",
    "        mean_pooled = []\n",
    "        for batch in tqdm(batchs, total=len(batchs), desc='Training...'):\n",
    "            mean_pooled.append(self.transform(batch))\n",
    "        mean_pooled_tensor = torch.tensor(\n",
    "            len(data), dtype=float).to(self.small_device)\n",
    "        mean_pooled = torch.cat(mean_pooled, out=mean_pooled_tensor)\n",
    "        self.embed_mat = mean_pooled\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(\n",
    "            -1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def transform(self, data):\n",
    "        if 'str' in data.__class__.__name__:\n",
    "            data = [data]\n",
    "        data = list(data)\n",
    "        token_dict = self.tokenizer(\n",
    "            data,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\")\n",
    "        token_dict = self.to(token_dict, self.device)\n",
    "        with torch.no_grad():\n",
    "            token_embed = self.model(**token_dict)\n",
    "        # each of the 512 token has a 768 or 384-d vector depends on model)\n",
    "        attention_mask = token_dict['attention_mask']\n",
    "        # average pooling of masked embeddings\n",
    "        mean_pooled = self.mean_pooling(\n",
    "            token_embed, attention_mask)\n",
    "        mean_pooled = mean_pooled.to(self.small_device)\n",
    "        return mean_pooled\n",
    "    \n",
    "    def to(self, data: dict, device: str):\n",
    "        \"\"\"Send all values to device by calling v.to(device)\"\"\"\n",
    "        data = {k: v.to(device) for k, v in data.items()}\n",
    "        return data\n",
    "\n",
    "    def predict(self, in_sentence, topk=3):\n",
    "        input_vec = self.transform(in_sentence)\n",
    "        mat = cosine_similarity(input_vec, self.embed_mat)\n",
    "        # best cos sim for each token independantly\n",
    "        best_index = extract_best_indices(mat, topk=topk)\n",
    "        return best_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d821c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a2bfaed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Tahera/.cache/huggingface/hub/models--sentence-transformers--paraphrase-MiniLM-L6-v2/refs/main'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Tahera/Desktop/ANLY 580/Project/movie_recommendation/recommendation_movie.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# CPU training\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bert_model \u001b[39m=\u001b[39m BertModel(model_name\u001b[39m=\u001b[39;49mMODEL_NAME, batch_size\u001b[39m=\u001b[39;49mBERT_BATCH_SIZE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m bert_model\u001b[39m.\u001b[39membed(df\u001b[39m.\u001b[39msentence\u001b[39m.\u001b[39mvalues)\n",
      "\u001b[1;32m/Users/Tahera/Desktop/ANLY 580/Project/movie_recommendation/recommendation_movie.ipynb Cell 36\u001b[0m in \u001b[0;36mBertModel.__init__\u001b[0;34m(self, model_name, device, small_memory, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmall_device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m small_memory \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_pretrained_model()\n",
      "\u001b[1;32m/Users/Tahera/Desktop/ANLY 580/Project/movie_recommendation/recommendation_movie.ipynb Cell 36\u001b[0m in \u001b[0;36mBertModel.load_pretrained_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pretrained_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Tahera/Desktop/ANLY%20580/Project/movie_recommendation/recommendation_movie.ipynb#X45sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:560\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    559\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    562\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenizer_config[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:412\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39mLoads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mtokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    411\u001b[0m commit_hash \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 412\u001b[0m resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    413\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    414\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    415\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    416\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m     resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    418\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    419\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    421\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    422\u001b[0m     _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    423\u001b[0m     _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    424\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/anly503/lib/python3.9/site-packages/huggingface_hub/file_download.py:1136\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     ref_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(storage_folder, \u001b[39m\"\u001b[39m\u001b[39mrefs\u001b[39m\u001b[39m\"\u001b[39m, revision)\n\u001b[0;32m-> 1136\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(ref_path) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m   1137\u001b[0m         commit_hash \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1139\u001b[0m pointer_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m   1140\u001b[0m     storage_folder, \u001b[39m\"\u001b[39m\u001b[39msnapshots\u001b[39m\u001b[39m\"\u001b[39m, commit_hash, relative_filename\n\u001b[1;32m   1141\u001b[0m )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Tahera/.cache/huggingface/hub/models--sentence-transformers--paraphrase-MiniLM-L6-v2/refs/main'"
     ]
    }
   ],
   "source": [
    "# CPU training\n",
    "bert_model = BertModel(model_name=MODEL_NAME, batch_size=BERT_BATCH_SIZE)\n",
    "bert_model.embed(df.sentence.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU training\n",
    "bert_model_gpu = BertModel(model_name=MODEL_NAME, batch_size=BERT_BATCH_SIZE, device='cuda')\n",
    "bert_model_gpu.transform(df.sentence.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afff360",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bff84df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim (1, 20000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>genres</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14496</th>\n",
       "      <td>The Princess and the Frog</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 10751, 'name': 'Family'}, {'id': 16, 'name': 'Animation'}, {'id': 10402, 'name': 'Music'}]</td>\n",
       "      <td>A waitress, desperate to fulfill her dreams as a restaurant owner, is set on a journey to turn a frog prince back into a human being, but she has to do face the same problem after she kisses him.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>WiseGirls</td>\n",
       "      <td>[{'id': 28, 'name': 'Action'}, {'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Drama'}, {'id': 53, 'name': 'Thriller'}]</td>\n",
       "      <td>A new waitress working at an Italian restaurant in New York City finds herself entangled in a mob-run underworld of drug dealing and murder.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11700</th>\n",
       "      <td>Fauteuils d'orchestre</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]</td>\n",
       "      <td>A young woman arrives in Paris where she finds a job as a waitress in bar next on Avenue Montaigne that caters to the surrounding theaters and the wealthy inhabitants of the area. She will meet a pianist, a famous actress and a great art collector, and become acquainted with the \"luxurious\" world her grandmother has told her about since her childhood.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  original_title  \\\n",
       "14496  The Princess and the Frog   \n",
       "5873                   WiseGirls   \n",
       "11700      Fauteuils d'orchestre   \n",
       "\n",
       "                                                                                                                                     genres  \\\n",
       "14496  [{'id': 10749, 'name': 'Romance'}, {'id': 10751, 'name': 'Family'}, {'id': 16, 'name': 'Animation'}, {'id': 10402, 'name': 'Music'}]   \n",
       "5873              [{'id': 28, 'name': 'Action'}, {'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Drama'}, {'id': 53, 'name': 'Thriller'}]   \n",
       "11700                                         [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'name': 'Drama'}, {'id': 10749, 'name': 'Romance'}]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                sentence  \n",
       "14496                                                                                                                                                                A waitress, desperate to fulfill her dreams as a restaurant owner, is set on a journey to turn a frog prince back into a human being, but she has to do face the same problem after she kisses him.  \n",
       "5873                                                                                                                                                                                                                        A new waitress working at an Italian restaurant in New York City finds herself entangled in a mob-run underworld of drug dealing and murder.  \n",
       "11700  A young woman arrives in Paris where she finds a job as a waitress in bar next on Avenue Montaigne that caters to the surrounding theaters and the wealthy inhabitants of the area. She will meet a pianist, a famous actress and a great art collector, and become acquainted with the \"luxurious\" world her grandmother has told her about since her childhood.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_sentence = 'the story of a waitress'\n",
    "indices = bert_model_gpu.predict(query_sentence)\n",
    "display(df[['original_title', 'genres', 'sentence']].iloc[indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f747f4033574bc018cd224c8390eda22f5cf19741325755a1e4dd992a19a8100"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
